{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install the model:\n",
    "`! pip install ollama llama-index-llms-ollama`\n",
    "`! sudo snap install ollama`\n",
    "`! ollama pull llama3`\n",
    "\n",
    "### After downloading the base model and creating the modelfile, we create the parametrised model for our task:\n",
    "`! ollama create [model name] -f [modelfile name]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "from lib.dataset_utils import load_twitter_data_cleaned, load_goemotions_cleaned\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score, jaccard_score, f1_score, classification_report\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "_, _, twitter_test = load_twitter_data_cleaned() \n",
    "twitter_emotions = \"\"\"'joy', 'sadness','anger', 'fear', 'love', 'surprise'\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Goemotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import json\n",
    "label_mapping_path = \"./dataset/GoEmotionsSplit/label_mapping.json\"\n",
    "_, _, goemotions_test = load_goemotions_cleaned()\n",
    "json1_file = open(label_mapping_path)\n",
    "json1_str = json1_file.read()\n",
    "json1_data = json.loads(json1_str)\n",
    "goemotions_emotions = str(json1_data.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "TWITTER_BASE_PROMPT = \"\"\"<|start_header_id|>system<|end_header_id|> Classify the sentences. Choose ONLY ONE EMOTION among the following: \"\"\" + twitter_emotions \n",
    "\n",
    "SAMPLES = \"\"\" \n",
    "text: 1. i left with my bouquet of red and yellow tulips under my arm feeling slightly more optimistic than when i arrived\n",
    "2. im updating my blog because i feel shitty\n",
    "\n",
    "answer:{\n",
    "    \"1\": \"joy\"\n",
    "    \"2\": \"sadness\"\n",
    "    }\n",
    "<|eot_id|>\"\"\"\n",
    "\n",
    "GOEMOTIONS_SINGLE_BASE_PROMPT = \"\"\"<|start_header_id|>system<|end_header_id|> Classify the sentences. Choose ONLY ONE EMOTION among the following: \"\"\" + goemotions_emotions\n",
    "\n",
    "GOEMOTIONS_MULTI_BASE_PROMPT = \"\"\"<|start_header_id|>system<|end_header_id|> Classify the sentences. Choose a maximum of three emotions among the following: \"\"\" + goemotions_emotions\n",
    "\n",
    "SAMPLES_STRING = \"\"\"Here are some samples:\"\"\"\n",
    "\n",
    "TERMINATOR_STRING = \"\"\"<|eot_id|>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "class Llama3():\n",
    "    def __init__(self, name, timeout = 1000.0, scores={}):\n",
    "        self.model = Ollama(model=name, request_timeout=timeout)\n",
    "        self.scores = scores\n",
    "\n",
    "    def predict(self, dataset_type, test, samples = None, batch_dim = 8, single_label = True, progress_bar = False):\n",
    "        # executes classification task on model\n",
    "        # k : number of shots (examples)\n",
    "        # emotions : list of emotions to be classified \n",
    "        # train : training data to fetch examples from\n",
    "        # test : test data to classify\n",
    "        # batch_dim : size of batch per prompt\n",
    "        # single_label : whether to classify single label or multi-label\n",
    "\n",
    "        test_loader = DataLoader(test, batch_size = batch_dim, shuffle = False)\n",
    "        predictions = []\n",
    "        base_prompt = self.generate_base_prompt(dataset_type, samples, single_label) # to avoid recreating the prompt from scratch at every batch\n",
    "        for data in tqdm(test_loader, disable=not progress_bar):\n",
    "            batch_prompt = self.add_test_data_to_prompt(base_prompt, data[0])\n",
    "            predictions.append(self.classify_batch(batch_prompt))\n",
    "        predictions = flatten(predictions)\n",
    "        if len(predictions) == len(test):\n",
    "            results = self.evaluate(test.targets, predictions, self.scores)\n",
    "            print(results)\n",
    "        else:\n",
    "            print(f\"Error: predictions and test data do not match: pred: {len(predictions)} vs test:{len(test)}\")\n",
    "\n",
    "    def generate_base_prompt(self, dataset_type, samples, single_label = True):\n",
    "        # spaghetti code but python 3.8 has no switch-case \n",
    "        if dataset_type == \"twitter\":\n",
    "            if samples:\n",
    "                return TWITTER_BASE_PROMPT + SAMPLES_STRING + samples\n",
    "            return TWITTER_BASE_PROMPT\n",
    "        \n",
    "        if dataset_type == \"goemotions\":\n",
    "            if single_label:\n",
    "                if samples:\n",
    "                    return GOEMOTIONS_SINGLE_BASE_PROMPT + SAMPLES_STRING + samples\n",
    "                return GOEMOTIONS_SINGLE_BASE_PROMPT    \n",
    "            if samples:\n",
    "                return GOEMOTIONS_MULTI_BASE_PROMPT + SAMPLES_STRING + samples\n",
    "        return GOEMOTIONS_MULTI_BASE_PROMPT \n",
    "                \n",
    "    def add_test_data_to_prompt(self, prompt, test):\n",
    "        # appends data to classify to base prompt\n",
    "        for index, row in enumerate(test):\n",
    "            prompt += (str(index) + '. ' + row + '\\n')\n",
    "        prompt += TERMINATOR_STRING\n",
    "        return prompt\n",
    "\n",
    "    def classify_batch(self, prompt):\n",
    "        # classify batch of data\n",
    "        # response is formatted as JSON\n",
    "        response = self.model.complete(prompt).text\n",
    "        emotions = self.extract_emotions(response)\n",
    "        return list(emotions.values())\n",
    "    \n",
    "    def evaluate(self, targets, predictions, scores):\n",
    "        # TODO: fix one hot encoding and handle 'other' case\n",
    "        # evaluate the model\n",
    "        ohe = OneHotEncoder()\n",
    "        predictions = ohe.transform(predictions)\n",
    "        predictions = pd.DataFrame(predictions, columns = ohe.categories_) # 'other' if emotion not in the allowed ones)\n",
    "        scores = {name: score(targets, predictions) for name, score in scores.items()}\n",
    "        return scores\n",
    "\n",
    "    def extract_emotions(self, answers):\n",
    "        answers[answers.find('{'):]\n",
    "        return json.loads(answers)\n",
    "\n",
    "\n",
    "\n",
    "class Llama_EmotionsData(Dataset):\n",
    "    def __init__(self, dataframe) -> None:\n",
    "        self.text = dataframe['text']\n",
    "        self.targets = dataframe.drop(columns=['text']).to_numpy()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.text[index], self.targets[index]\n",
    "\n",
    "\n",
    "def flatten(xss):\n",
    "    return [x for xs in xss for x in xs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:50<00:00, 25.32s/it]\n"
     ]
    },
    {
     "ename": "NotFittedError",
     "evalue": "This OneHotEncoder instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m llama3 \u001b[38;5;241m=\u001b[39m Llama3(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParametrisedLlama3\u001b[39m\u001b[38;5;124m\"\u001b[39m, scores \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m: accuracy_score})\n\u001b[1;32m      2\u001b[0m twitter_test_dataset \u001b[38;5;241m=\u001b[39m Llama_EmotionsData(twitter_test[:\u001b[38;5;241m16\u001b[39m])\n\u001b[0;32m----> 3\u001b[0m \u001b[43mllama3\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtwitter\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtwitter_test_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamples\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mSAMPLES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msingle_label\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 23\u001b[0m, in \u001b[0;36mLlama3.predict\u001b[0;34m(self, dataset_type, test, samples, batch_dim, single_label, progress_bar)\u001b[0m\n\u001b[1;32m     21\u001b[0m predictions \u001b[38;5;241m=\u001b[39m flatten(predictions)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(predictions) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(test):\n\u001b[0;32m---> 23\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscores\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28mprint\u001b[39m(results)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[5], line 61\u001b[0m, in \u001b[0;36mLlama3.evaluate\u001b[0;34m(self, targets, predictions, scores)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate\u001b[39m(\u001b[38;5;28mself\u001b[39m, targets, predictions, scores):\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;66;03m# evaluate the model\u001b[39;00m\n\u001b[1;32m     60\u001b[0m     ohe \u001b[38;5;241m=\u001b[39m OneHotEncoder()\n\u001b[0;32m---> 61\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m \u001b[43mohe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(predictions, columns \u001b[38;5;241m=\u001b[39m ohe\u001b[38;5;241m.\u001b[39mcategories_) \u001b[38;5;66;03m# 'other' if emotion not in the allowed ones)\u001b[39;00m\n\u001b[1;32m     63\u001b[0m     scores \u001b[38;5;241m=\u001b[39m {name: score(targets, predictions) \u001b[38;5;28;01mfor\u001b[39;00m name, score \u001b[38;5;129;01min\u001b[39;00m scores\u001b[38;5;241m.\u001b[39mitems()}\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/utils/_set_output.py:157\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 157\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    160\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    161\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    162\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    163\u001b[0m         )\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/preprocessing/_encoders.py:1013\u001b[0m, in \u001b[0;36mOneHotEncoder.transform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    994\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransform\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m    995\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    996\u001b[0m \u001b[38;5;124;03m    Transform X using one-hot encoding.\u001b[39;00m\n\u001b[1;32m    997\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1011\u001b[0m \u001b[38;5;124;03m        returned.\u001b[39;00m\n\u001b[1;32m   1012\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1013\u001b[0m     \u001b[43mcheck_is_fitted\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1014\u001b[0m     transform_output \u001b[38;5;241m=\u001b[39m _get_output_config(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransform\u001b[39m\u001b[38;5;124m\"\u001b[39m, estimator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdense\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1015\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m transform_output \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpandas\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparse_output:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/utils/validation.py:1461\u001b[0m, in \u001b[0;36mcheck_is_fitted\u001b[0;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[1;32m   1458\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m is not an estimator instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (estimator))\n\u001b[1;32m   1460\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_fitted(estimator, attributes, all_or_any):\n\u001b[0;32m-> 1461\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NotFittedError(msg \u001b[38;5;241m%\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(estimator)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m})\n",
      "\u001b[0;31mNotFittedError\u001b[0m: This OneHotEncoder instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator."
     ]
    }
   ],
   "source": [
    "llama3 = Llama3(\"ParametrisedLlama3\", scores = {\"accuracy\": accuracy_score})\n",
    "twitter_test_dataset = Llama_EmotionsData(twitter_test[:16])\n",
    "llama3.predict(\"twitter\", twitter_test_dataset, samples = SAMPLES, single_label = True, progress_bar = True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
