{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install the model:\n",
    "`! pip install ollama llama-index-llms-ollama`\n",
    "`! sudo snap install ollama`\n",
    "`! ollama pull llama3`\n",
    "\n",
    "### After downloading the base model and creating the modelfile, we create the parametrised model for our task:\n",
    "`! ollama create [model name] -f [modelfile name]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "from lib.dataset_utils import load_twitter_data_cleaned, load_goemotions_cleaned\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, LabelBinarizer\n",
    "from sklearn.metrics import accuracy_score, jaccard_score, f1_score, classification_report\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "_, _, twitter_test = load_twitter_data_cleaned() \n",
    "twitter_emotions = \"\"\"'joy', 'sadness','anger', 'fear', 'love', 'surprise'\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Goemotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import json\n",
    "label_mapping_path = \"./dataset/GoEmotionsSplit/label_mapping.json\"\n",
    "_, _, goemotions_test = load_goemotions_cleaned()\n",
    "json1_file = open(label_mapping_path)\n",
    "json1_str = json1_file.read()\n",
    "json1_data = json.loads(json1_str)\n",
    "goemotions_emotions = str(json1_data.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "TWITTER_BASE_PROMPT = \"\"\"<|start_header_id|>system<|end_header_id|> Classify the sentences. Choose ONLY ONE EMOTION among the following: \"\"\" + twitter_emotions \n",
    "\n",
    "SAMPLES = \"\"\" \n",
    "text: 1. i left with my bouquet of red and yellow tulips under my arm feeling slightly more optimistic than when i arrived\n",
    "2. im updating my blog because i feel shitty\n",
    "\n",
    "answer:{\n",
    "    \"1\": \"joy\"\n",
    "    \"2\": \"sadness\"\n",
    "    }\n",
    "<|eot_id|>\"\"\"\n",
    "\n",
    "GOEMOTIONS_SINGLE_BASE_PROMPT = \"\"\"<|start_header_id|>system<|end_header_id|> Classify the sentences. Choose ONLY ONE EMOTION among the following: \"\"\" + goemotions_emotions\n",
    "\n",
    "GOEMOTIONS_MULTI_BASE_PROMPT = \"\"\"<|start_header_id|>system<|end_header_id|> Classify the sentences. Choose a maximum of three emotions among the following: \"\"\" + goemotions_emotions\n",
    "\n",
    "SAMPLES_STRING = \"\"\"Here are some samples:\"\"\"\n",
    "\n",
    "TERMINATOR_STRING = \"\"\"<|eot_id|>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "class Llama3():\n",
    "    def __init__(self, name, timeout = 1000.0, scores={}):\n",
    "        self.model = Ollama(model=name, request_timeout=timeout)\n",
    "        self.scores = scores\n",
    "\n",
    "    def predict(self, dataset_type, test, samples = None, batch_dim = 8, single_label = True, progress_bar = False):\n",
    "        # dataset_type : twitter or goemotions, probably just need to pass emotions list directly instead \n",
    "        # test : test data to classify\n",
    "        # samples : samples to show in prompt (taken from training data?), for now are hardcoded\n",
    "        test_loader = DataLoader(test, batch_size = batch_dim, shuffle = False)\n",
    "        predictions = []\n",
    "        base_prompt = self.generate_base_prompt(dataset_type, samples, single_label) # to avoid recreating the prompt from scratch at every batch\n",
    "        for data in tqdm(test_loader, disable=not progress_bar):\n",
    "            batch_prompt = self.add_test_data_to_prompt(base_prompt, data[0])\n",
    "            predictions.append(self.classify_batch(batch_prompt))\n",
    "        predictions = flatten(predictions)\n",
    "        if len(predictions) == len(test):\n",
    "            results = self.evaluate(test.targets, predictions, self.scores)\n",
    "            print(results)\n",
    "        else:\n",
    "            print(f\"Error: predictions and test data do not match: pred: {len(predictions)} vs test:{len(test)}\")\n",
    "\n",
    "    def generate_base_prompt(self, dataset_type, samples, single_label = True):\n",
    "        # bad but python 3.8 has no switch-case \n",
    "        if dataset_type == \"twitter\":\n",
    "            if samples:\n",
    "                return TWITTER_BASE_PROMPT + SAMPLES_STRING + samples\n",
    "            return TWITTER_BASE_PROMPT\n",
    "        \n",
    "        if dataset_type == \"goemotions\":\n",
    "            if single_label:\n",
    "                if samples:\n",
    "                    return GOEMOTIONS_SINGLE_BASE_PROMPT + SAMPLES_STRING + samples\n",
    "                return GOEMOTIONS_SINGLE_BASE_PROMPT    \n",
    "            if samples:\n",
    "                return GOEMOTIONS_MULTI_BASE_PROMPT + SAMPLES_STRING + samples\n",
    "        return GOEMOTIONS_MULTI_BASE_PROMPT \n",
    "                \n",
    "    def add_test_data_to_prompt(self, prompt, test):\n",
    "        # appends data to classify to base prompt\n",
    "        for index, row in enumerate(test):\n",
    "            prompt += (str(index) + '. ' + row + '\\n')\n",
    "        prompt += TERMINATOR_STRING\n",
    "        return prompt\n",
    "\n",
    "    def classify_batch(self, prompt):\n",
    "        # classify batch of data\n",
    "        # response is formatted as JSON\n",
    "        response = self.model.complete(prompt).text\n",
    "        emotions = self.extract_emotions(response)\n",
    "        return list(emotions.values())\n",
    "    \n",
    "    def evaluate(self, targets, predictions, scores, single_label = True):\n",
    "        # TODO: fix one hot encoding and handle 'other' case\n",
    "        # evaluate the model\n",
    "        if single_label:\n",
    "            lb = LabelBinarizer()\n",
    "        else:\n",
    "            lb = MultiLabelBinarizer()\n",
    "        bin_predictions = lb.fit_transform(predictions)\n",
    "        bin_predictions = pd.DataFrame(bin_predictions, columns = lb.classes_) # 'other' if emotion not in the allowed ones)\n",
    "        scores = {name: score(targets, bin_predictions) for name, score in scores.items()}\n",
    "        return scores\n",
    "\n",
    "    def extract_emotions(self, answers):\n",
    "        # extracts emotions from JSON response \n",
    "        answers[answers.find('{'):] # skips optional text before JSON\n",
    "        return json.loads(answers)\n",
    "\n",
    "def accuracy(targets, predictions):\n",
    "    return accuracy_score(targets, predictions)\n",
    "def jaccard(targets, predictions):\n",
    "    return jaccard_score(targets, predictions, average='micro', zero_division=0)\n",
    "def jaccard_samples(targets, predictions):\n",
    "    return jaccard_score(targets, predictions, average='samples', zero_division=0)\n",
    "def f1(targets, predictions):\n",
    "    return f1_score(targets, predictions, average='macro')\n",
    "def f1_micro(targets, predictions):\n",
    "    return f1_score(targets, predictions, average='micro')\n",
    "\n",
    "\n",
    "def flatten(xss):\n",
    "    # flattens list of lists into a single list\n",
    "    return [x for xs in xss for x in xs]\n",
    "\n",
    "class Llama_EmotionsData(Dataset):\n",
    "    def __init__(self, dataframe) -> None:\n",
    "        self.text = dataframe['text']\n",
    "        self.targets = dataframe.drop(columns=['text']).to_numpy()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.text[index], self.targets[index]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [02:11<00:00, 32.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.46875, 'jaccard': 0.30612244897959184, 'jaccard_samples': 0.46875, 'f1': 0.40840455840455836, 'f1_micro': 0.46875}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "llama3 = Llama3(\"ParametrisedLlama3\", scores = {\"accuracy\": accuracy, \"jaccard\": jaccard, \n",
    "                                                \"jaccard_samples\":jaccard_samples, \"f1\": f1, \"f1_micro\": f1_micro})\n",
    "twitter_test_dataset = Llama_EmotionsData(twitter_test[:32])\n",
    "llama3.predict(\"twitter\", twitter_test_dataset, samples = SAMPLES, single_label = True, progress_bar = True)\n",
    "lb = LabelBinarizer()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
