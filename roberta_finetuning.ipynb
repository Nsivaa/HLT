{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries needed\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import seaborn as sns\n",
    "import transformers\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import RobertaModel, RobertaTokenizer\n",
    "import logging\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "from torch import cuda\n",
    "from lib.dataset_utils import *\n",
    "from lib.plot_utils import *\n",
    "from sklearn.metrics import accuracy_score, jaccard_score, f1_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Minibatch\n",
    "- learning rate\n",
    "- momentum\n",
    "- regularization\n",
    "- dropout?\n",
    "- topologia\n",
    "- optimizer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining some key variables that will be used later on in the training\n",
    "DATASET_NAME = DatasetEnum.GoEmotionsCleaned\n",
    "MINIBATCH_SIZE = 16\n",
    "EPOCHS = 1\n",
    "LEARNING_RATE = 5e-05\n",
    "FROZEN_LAYERS = 9\n",
    "loader_params = {'batch_size': MINIBATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "# Creating the loss function and optimizer\n",
    "loss_function = torch.nn.BCEWithLogitsLoss()\n",
    "train_df, val_df, test_df = load_dataset(DATASET_NAME)\n",
    "MAX_LEN = compute_max_tokens([train_df, val_df, test_df], RobertaTokenizer.from_pretrained('roberta-base'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>admiration</th>\n",
       "      <th>amusement</th>\n",
       "      <th>disapproval</th>\n",
       "      <th>disgust</th>\n",
       "      <th>embarrassment</th>\n",
       "      <th>excitement</th>\n",
       "      <th>fear</th>\n",
       "      <th>gratitude</th>\n",
       "      <th>grief</th>\n",
       "      <th>...</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>neutral</th>\n",
       "      <th>annoyance</th>\n",
       "      <th>approval</th>\n",
       "      <th>caring</th>\n",
       "      <th>confusion</th>\n",
       "      <th>curiosity</th>\n",
       "      <th>desire</th>\n",
       "      <th>disappointment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My favourite food is anything I didn't have to...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Now if he does off himself, everyone will thin...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>WHY THE FUCK IS BAYLESS ISOING</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>To make her feel threatened</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dirty Southern Wankers</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>OmG pEyToN iSn'T gOoD eNoUgH tO hElP uS iN tHe...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Yes I heard abt the f bombs! That has to be wh...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>We need more boards and to create a bit more s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Damn youtube and outrage drama is super lucrat...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>It might be linked to the trust factor of your...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  admiration  amusement  \\\n",
       "0  My favourite food is anything I didn't have to...           0          0   \n",
       "1  Now if he does off himself, everyone will thin...           0          0   \n",
       "2                     WHY THE FUCK IS BAYLESS ISOING           0          0   \n",
       "3                        To make her feel threatened           0          0   \n",
       "4                             Dirty Southern Wankers           0          0   \n",
       "5  OmG pEyToN iSn'T gOoD eNoUgH tO hElP uS iN tHe...           0          0   \n",
       "6  Yes I heard abt the f bombs! That has to be wh...           0          0   \n",
       "7  We need more boards and to create a bit more s...           0          0   \n",
       "8  Damn youtube and outrage drama is super lucrat...           1          0   \n",
       "9  It might be linked to the trust factor of your...           0          0   \n",
       "\n",
       "   disapproval  disgust  embarrassment  excitement  fear  gratitude  grief  \\\n",
       "0            0        0              0           0     0          0      0   \n",
       "1            0        0              0           0     0          0      0   \n",
       "2            0        0              0           0     0          0      0   \n",
       "3            0        0              0           0     1          0      0   \n",
       "4            0        0              0           0     0          0      0   \n",
       "5            0        0              0           0     0          0      0   \n",
       "6            0        0              0           0     0          1      0   \n",
       "7            0        0              0           0     0          0      0   \n",
       "8            0        0              0           0     0          0      0   \n",
       "9            0        0              0           0     0          0      0   \n",
       "\n",
       "   ...  sadness  surprise  neutral  annoyance  approval  caring  confusion  \\\n",
       "0  ...        0         0        1          0         0       0          0   \n",
       "1  ...        0         0        1          0         0       0          0   \n",
       "2  ...        0         0        0          0         0       0          0   \n",
       "3  ...        0         0        0          0         0       0          0   \n",
       "4  ...        0         0        0          1         0       0          0   \n",
       "5  ...        0         1        0          0         0       0          0   \n",
       "6  ...        0         0        0          0         0       0          0   \n",
       "7  ...        0         0        0          0         0       0          0   \n",
       "8  ...        0         0        0          0         0       0          0   \n",
       "9  ...        0         0        1          0         0       0          0   \n",
       "\n",
       "   curiosity  desire  disappointment  \n",
       "0          0       0               0  \n",
       "1          0       0               0  \n",
       "2          0       0               0  \n",
       "3          0       0               0  \n",
       "4          0       0               0  \n",
       "5          0       0               0  \n",
       "6          0       0               0  \n",
       "7          0       1               0  \n",
       "8          0       0               0  \n",
       "9          0       0               0  \n",
       "\n",
       "[10 rows x 29 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO cross validation su topologia\n",
    "class RobertaClass(torch.nn.Module):\n",
    "    def __init__(self, n_classes, frozen_layers=-1):\n",
    "        super(RobertaClass, self).__init__()\n",
    "        self.l1 = RobertaModel.from_pretrained(\"roberta-base\")\n",
    "        if frozen_layers != -1:\n",
    "            for param in self.l1.embeddings.parameters():\n",
    "                param.requires_grad = False\n",
    "            for i in range(frozen_layers):\n",
    "                for param in self.l1.encoder.layer[i].parameters():\n",
    "                    param.requires_grad = False\n",
    "        self.pre_classifier = torch.nn.Linear(768, 768)#TODO add_module?\n",
    "        self.dropout = torch.nn.Dropout(0.3)\n",
    "        self.classifier = torch.nn.Linear(768, n_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        hidden_state = output_1[0]#TODO ???\n",
    "        pooler = hidden_state[:, 0]\n",
    "        pooler = self.pre_classifier(pooler)\n",
    "        pooler = torch.nn.ReLU()(pooler)\n",
    "        pooler = self.dropout(pooler)\n",
    "        output = self.classifier(pooler)\n",
    "        return output\n",
    "    \n",
    "    def get_out_dim(self):\n",
    "        return self.classifier.out_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_params(optimizer=torch.optim.Adam,\n",
    "                        tokenizer=RobertaTokenizer.from_pretrained('roberta-base', truncation=True, do_lower_case=True),\n",
    "                        tokenizer_max_len=None,\n",
    "                        loader_params={'batch_size': 8, 'shuffle': True, 'num_workers': 0},\n",
    "                        loss_function=torch.nn.BCEWithLogitsLoss(),\n",
    "                        epochs=1,\n",
    "                        learning_rate=1e-05,\n",
    "                        val_patience=1,\n",
    "                        clip_grad_norm=-1):\n",
    "    return {\n",
    "        'optimizer': optimizer,\n",
    "        'tokenizer': tokenizer,\n",
    "        'tokenizer_max_len': tokenizer_max_len,\n",
    "        'loader_params': loader_params,\n",
    "        'loss_function': loss_function,\n",
    "        'epochs': epochs,\n",
    "        'learning_rate': learning_rate,\n",
    "        'val_patience': val_patience,\n",
    "        'clip_grad_norm': clip_grad_norm\n",
    "    }\n",
    "\n",
    "class SimpleModelInterface:\n",
    "    def __init__(self, \n",
    "                 model: torch.nn.Module, \n",
    "                 scores={},\n",
    "                 model_params_dict=create_model_params()):\n",
    "        self.model = model\n",
    "        self.params = model_params_dict\n",
    "        self.optimizer = model_params_dict['optimizer'](params=model.parameters(), lr=model_params_dict['learning_rate'])\n",
    "        self.scores = scores\n",
    "        self.train_scores = {name: [] for name in scores.keys()}\n",
    "        self.train_loss = []\n",
    "        self.val_scores = {name: [] for name in scores.keys()}\n",
    "        self.val_loss = []\n",
    "        self.device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "        model.to(self.device)\n",
    "\n",
    "    def _train(self, training_loader, validation_loader=None, save_path=None):\n",
    "        self.model.train()\n",
    "        #TODO usare confusion matrix?\n",
    "        cur_patience = self.params['val_patience']\n",
    "        best_val_loss = np.inf\n",
    "        for _ in range(self.params['epochs']):\n",
    "            tr_loss = 0\n",
    "            predictions_acc = []\n",
    "            targets_acc = []\n",
    "            for _,data in tqdm(enumerate(training_loader, 0)):\n",
    "                ids = data['ids'].to(self.device, dtype = torch.long)\n",
    "                mask = data['mask'].to(self.device, dtype = torch.long)\n",
    "                token_type_ids = data['token_type_ids'].to(self.device, dtype = torch.long)\n",
    "                targets = data['targets'].to(self.device, dtype = torch.float)\n",
    "\n",
    "                outputs = self.model(ids, mask, token_type_ids)\n",
    "                loss = loss_function(outputs, targets)\n",
    "                tr_loss += loss.item()\n",
    "                # append predictions and targets\n",
    "                predictions_acc.extend(torch.sigmoid(outputs).detach().cpu().numpy())\n",
    "                targets_acc.extend(targets.detach().cpu().numpy())\n",
    "\n",
    "                # backward pass\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                if self.params['clip_grad_norm'] > -0:\n",
    "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.params['clip_grad_norm'])\n",
    "                self.optimizer.step()\n",
    "\n",
    "            # create numpy arrays\n",
    "            predictions_acc = np.array(predictions_acc)\n",
    "            targets_acc = np.array(targets_acc)\n",
    "            # calculate training scores\n",
    "            epoch_loss = tr_loss/len(training_loader)\n",
    "            self.train_loss.append(epoch_loss)\n",
    "            for name, score in self.scores.items():\n",
    "                self.train_scores[name].append(score(targets_acc, predictions_acc))\n",
    "\n",
    "            # calculate validation scores\n",
    "            if validation_loader is not None:\n",
    "                val_scores = self.evaluate(validation_loader)\n",
    "                for name, score in val_scores.items():\n",
    "                    self.val_scores[name].append(score)\n",
    "                self.val_loss.append(val_scores['loss'])\n",
    "                if val_scores['loss'] < best_val_loss:\n",
    "                    best_val_loss = val_scores['loss']\n",
    "                    cur_patience = self.params['val_patience']\n",
    "                    # save model\n",
    "                    if save_path is not None:\n",
    "                        torch.save(self.model, save_path)\n",
    "                else:\n",
    "                    cur_patience -= 1\n",
    "                if cur_patience == 0:\n",
    "                    break\n",
    "\n",
    "        # restore best model\n",
    "        if save_path is not None and validation_loader is not None:\n",
    "            self.model = torch.load(save_path)\n",
    "            self.model.to(self.device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def fit(self, training_df, validation_df=None):\n",
    "        training_loader = create_data_loader_from_dataframe(training_df, self.params['tokenizer'], self.params['tokenizer_max_len'], **self.params['loader_params'])\n",
    "        validation_loader = None\n",
    "        if validation_df is not None:\n",
    "            validation_loader = create_data_loader_from_dataframe(validation_df, self.params['tokenizer'], self.params['tokenizer_max_len'], **self.params['loader_params'])\n",
    "        self._train(training_loader, validation_loader)\n",
    "\n",
    "    def _predict(self, data_loader, compute_scores=False):\n",
    "        self.model.eval()\n",
    "        pred_loss=0\n",
    "        # initialize target and prediction matrices\n",
    "        predictions_acc = []\n",
    "        targets_acc = []\n",
    "        with torch.no_grad():\n",
    "            for _, data in tqdm(enumerate(data_loader, 0)):\n",
    "                ids = data['ids'].to(self.device, dtype = torch.long)\n",
    "                mask = data['mask'].to(self.device, dtype = torch.long)\n",
    "                token_type_ids = data['token_type_ids'].to(self.device, dtype=torch.long)\n",
    "                targets = data['targets'].to(self.device, dtype = torch.float)\n",
    "                outputs = self.model(ids, mask, token_type_ids).squeeze()\n",
    "                if compute_scores:\n",
    "                    # accumulate loss\n",
    "                    loss = loss_function(outputs, targets)\n",
    "                    pred_loss += loss.item()\n",
    "                # append predictions and targets\n",
    "                predictions_acc.extend(torch.sigmoid(outputs).detach().cpu().numpy())\n",
    "                targets_acc.extend(targets.detach().cpu().numpy())\n",
    "        return np.array(targets_acc), np.array(predictions_acc), pred_loss\n",
    "\n",
    "    def predict(self, testing_df):\n",
    "        testing_loader = create_data_loader_from_dataframe(testing_df, self.params['tokenizer'], self.params['tokenizer_max_len'], **self.params['loader_params'])\n",
    "        target, out, _ = self._predict(testing_loader, compute_scores=False)\n",
    "        return out, target\n",
    "\n",
    "    def evaluate(self, testing_df):\n",
    "        testing_loader = create_data_loader_from_dataframe(testing_df, self.params['tokenizer'], self.params['tokenizer_max_len'], **self.params['loader_params'])\n",
    "        targets_acc, predictions_acc, pred_loss = self._predict(testing_loader, compute_scores=True)\n",
    "        # calculate scores\n",
    "        scores = {name: score(targets_acc, predictions_acc) for name, score in self.scores.items()}\n",
    "        scores['loss'] = pred_loss/len(testing_loader)\n",
    "        return scores\n",
    "\n",
    "    def get_train_scores(self):\n",
    "        return self.train_scores\n",
    "    \n",
    "    def get_train_loss(self):\n",
    "        return self.train_loss\n",
    "\n",
    "    def get_val_scores(self):\n",
    "        return self.val_scores\n",
    "    \n",
    "    def get_val_loss(self):\n",
    "        return self.val_loss\n",
    "    \n",
    "    def save_model(self, model_path, vocabulary_path):\n",
    "        torch.save(self.model, model_path)\n",
    "        self.tokenizer.save_vocabulary(vocabulary_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_sigmoid_threshold(y_true, y_pred, metric_fun=accuracy_score, metric_params={}, is_maximization=True):\n",
    "    thresholds = np.arange(0, 1, 0.01)\n",
    "    scores = [metric_fun(y_true, y_pred > t, **metric_params) for t in thresholds]\n",
    "    best_threshold = thresholds[np.argmax(scores)] if is_maximization else thresholds[np.argmin(scores)]\n",
    "    return best_threshold, scores\n",
    "\n",
    "def plot_threshold_tuning(y_true, y_pred, metric_fun=accuracy_score, metric_params={}, plot=False, is_maximization=True, metric_name='Accuracy'):\n",
    "    best_threshold, scores = tune_sigmoid_threshold(y_true, y_pred, metric_fun, metric_params, is_maximization)\n",
    "    if plot:\n",
    "        plt.plot(np.arange(0, 1, 0.01), scores)\n",
    "        plt.xlabel('Threshold')\n",
    "        plt.ylabel(metric_name)\n",
    "        # get average type if provided\n",
    "        if 'average' in metric_params:\n",
    "            metric_name += f' ({metric_params[\"average\"]})'\n",
    "        plt.title(f'{metric_name} over sigmoid threshold')\n",
    "        plt.show()\n",
    "        print(f'Best threshold: {best_threshold}')\n",
    "        print(f'Best {metric_name}: {max(scores) if is_maximization else min(scores)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD = 0.5\n",
    "def accuracy(y_true, y_pred):# tuning implicitly done in score calculation :)\n",
    "    _, best_res = tune_sigmoid_threshold(y_true, y_pred, accuracy_score)\n",
    "    return best_res\n",
    "\n",
    "def jaccard(y_true, y_pred):\n",
    "    _, best_res = tune_sigmoid_threshold(y_true, y_pred, jaccard_score, {'average': 'macro'})\n",
    "    return best_res\n",
    "\n",
    "def jaccard_samples(y_true, y_pred):\n",
    "    _, best_res = tune_sigmoid_threshold(y_true, y_pred, jaccard_score, {'average': 'samples'})\n",
    "    return best_res\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    _, best_res = tune_sigmoid_threshold(y_true, y_pred, f1_score, {'average': 'macro'})\n",
    "    return best_res\n",
    "\n",
    "def f1_micro(y_true, y_pred):\n",
    "    _, best_res = tune_sigmoid_threshold(y_true, y_pred, f1_score, {'average': 'micro'})\n",
    "    return best_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#FIXME nclasses\n",
    "model = SimpleModelInterface(RobertaClass(train_df.shape[1]-1), {'accuracy': accuracy, 'jaccard_macro': jaccard, 'f1_macro': f1, 'jaccard_samples': jaccard_samples, 'f1_micro':f1_micro}, create_model_params(tokenizer_max_len=MAX_LEN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125it [00:47,  2.64it/s]\n"
     ]
    }
   ],
   "source": [
    "model.fit(train_df[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FIXME jaccard ed f1 non funzionano correttamente\n",
    "scores = model.evaluate(test_df[:1000])\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out, target = model.predict(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_threshold_tuning(target, out, plot=True)\n",
    "plot_threshold_tuning(target, out, plot=True, metric_params={'average':'micro'}, metric_fun=f1_score, metric_name='F1 Score')\n",
    "plot_threshold_tuning(target, out, plot=True, metric_params={'average':'macro'}, metric_fun=f1_score, metric_name='F1 Score')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HLT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
