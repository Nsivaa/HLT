{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries needed\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import seaborn as sns\n",
    "import transformers\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import RobertaModel, RobertaTokenizer\n",
    "import logging\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "from torch import cuda\n",
    "from lib.dataset_utils import *\n",
    "from lib.plot_utils import *\n",
    "from lib.scores import *\n",
    "from lib.models import *\n",
    "from lib.cross_validation import *\n",
    "from sklearn.metrics import accuracy_score, jaccard_score, f1_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining some key variables that will be used later on in the training\n",
    "DATASET = DatasetEnum.GoEmotionsCleaned\n",
    "DATASET_NAME = 'GoEmotions'\n",
    "MODEL_NAME = 'Roberta'\n",
    "CHECKPOINT_DIR = './checkpoints/' + DATASET_NAME + '/'\n",
    "CHECKPOINT_MODEL_FILE = CHECKPOINT_DIR + DATASET_NAME + '_' + MODEL_NAME + '.pth'\n",
    "MINIBATCH_SIZE = 16\n",
    "EPOCHS = 6\n",
    "LAMBDA = 1e-04\n",
    "LEARNING_RATE = 5e-05\n",
    "FROZEN_LAYERS = 9\n",
    "# Creating the loss function and optimizer\n",
    "loss_function = torch.nn.BCEWithLogitsLoss()\n",
    "train_df, val_df, test_df = load_dataset(DATASET)\n",
    "LABEL_COLS = train_df.columns[1:].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preliminary attempts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model parameters\n",
    "PARAMS = {'batch_size':MINIBATCH_SIZE, \n",
    "          'learning_rate':LEARNING_RATE, \n",
    "          'epochs':EPOCHS, \n",
    "          'loss_function':loss_function, \n",
    "          'regularization':LAMBDA,\n",
    "          'n_classes':len(LABEL_COLS),\n",
    "          'frozen_layers':FROZEN_LAYERS}\n",
    "# scores dictionary\n",
    "SCORES = {'accuracy': accuracy,\n",
    "          'jaccard_macro': jaccard,\n",
    "          'f1_macro': f1,\n",
    "          'jaccard_samples': jaccard_samples,\n",
    "          'f1_micro':f1_micro,\n",
    "          'membership':membership_score}\n",
    "# creating the model\n",
    "model = Roberta(SCORES,\n",
    "                PARAMS)\n",
    "model.fit(train_df, validation_df=val_df, progress_bar_epoch=True, progress_bar_step=False, checkpoint_path=CHECKPOINT_MODEL_FILE, checkpoint_score='f1_macro', checkpoint_score_maximize=True)\n",
    "model_analysis(model, val_df, LABEL_COLS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_to_explore = {\n",
    "    'learning_rate': [5e-05, 1e-05],\n",
    "    'regularization': [0, 1e-05],\n",
    "    'batch_size': [16, 32],\n",
    "    'epochs': [6],\n",
    "    'frozen_layers': [FROZEN_LAYERS],\n",
    "    'n_classes': [len(LABEL_COLS)],\n",
    "}\n",
    "RESULT_DIR = './results/' + DATASET_NAME + '/'\n",
    "RESULT_FILE = RESULT_DIR + DATASET_NAME + '_' + MODEL_NAME + '.csv'\n",
    "# create the grid search object\n",
    "grid_search = HoldOutCrossValidation(Roberta, SCORES, train_df, val_df, param_dict=params_to_explore, res_file=RESULT_FILE)\n",
    "# run the grid search\n",
    "grid_search.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the results\n",
    "results = grid_search.get_results()\n",
    "# get the best parameters and scores according to F1 macro\n",
    "best_info = grid_search.get_best_info('f1_macro')\n",
    "print(\"BEST MODEL INFO\")\n",
    "print(best_info)\n",
    "# print the results\n",
    "print(\"RESULTS\")\n",
    "print(results)\n",
    "print(\"BEST PARAMS\")\n",
    "BEST_PARAMS = grid_search.get_best_params('f1_macro')\n",
    "BEST_PARAMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model with the best parameters\n",
    "model = Roberta(SCORES, BEST_PARAMS)\n",
    "model.fit(train_df, validation_df=val_df, progress_bar_epoch=True, progress_bar_step=False, checkpoint_path=CHECKPOINT_MODEL_FILE, checkpoint_score='f1_macro', checkpoint_score_maximize=True)\n",
    "model_analysis(model, train_df, val_df, LABEL_COLS, test_df, checkpoint_path=CHECKPOINT_MODEL_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULT_FILE = RESULT_DIR + DATASET_NAME + '_' + MODEL_NAME + '_features.csv'\n",
    "TOP_N = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempt on grouped emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_MODEL_FILE = CHECKPOINT_DIR + DATASET_NAME + '_' + 'Grouped' + '_' + MODEL_NAME + '.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map dataset emotions to twitter\n",
    "train_df = goemotions_apply_emotion_mapping(train_df)\n",
    "val_df = goemotions_apply_emotion_mapping(val_df)\n",
    "test_df = goemotions_apply_emotion_mapping(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model analysis with twitter mapped emotions\n",
    "model = Roberta(SCORES, BEST_PARAMS)\n",
    "model_analysis(model, train_df, val_df, LABEL_COLS, test_df, checkpoint_path=CHECKPOINT_MODEL_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importance on grouped dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULT_FILE = RESULT_DIR + DATASET_NAME + '_' + 'Grouped' + '_' + MODEL_NAME + '_features.csv'\n",
    "TOP_N = 30"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HLT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
