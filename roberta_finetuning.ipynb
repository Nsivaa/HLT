{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers_interpret'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RobertaModel, RobertaTokenizer\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers_interpret\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SequenceClassificationExplainer\n\u001b[0;32m     13\u001b[0m logging\u001b[38;5;241m.\u001b[39mbasicConfig(level\u001b[38;5;241m=\u001b[39mlogging\u001b[38;5;241m.\u001b[39mERROR)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cuda\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'transformers_interpret'"
     ]
    }
   ],
   "source": [
    "# Importing the libraries needed\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import seaborn as sns\n",
    "import transformers\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import RobertaModel, RobertaTokenizer\n",
    "import logging\n",
    "from transformers_interpret import MultiLabelClassificationExplainer\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "from torch import cuda\n",
    "from lib.dataset_utils import *\n",
    "from lib.plot_utils import *\n",
    "from lib.models import *\n",
    "from lib.cross_validation import *\n",
    "from sklearn.metrics import accuracy_score, jaccard_score, f1_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining some key variables that will be used later on in the training\n",
    "DATASET_NAME = DatasetEnum.GoEmotionsCleaned\n",
    "DATASET_NAME = 'GoEmotions'\n",
    "MODEL_NAME = 'Roberta'\n",
    "CHECKPOINT_DIR = './checkpoints/' + DATASET_NAME + '/'\n",
    "CHECKPOINT_MODEL_FILE = CHECKPOINT_DIR + DATASET_NAME + '_' + MODEL_NAME + '.pth'\n",
    "MINIBATCH_SIZE = 16\n",
    "EPOCHS = 6\n",
    "LAMBDA = 1e-04\n",
    "LEARNING_RATE = 5e-05\n",
    "FROZEN_LAYERS = 9\n",
    "# Creating the loss function and optimizer\n",
    "loss_function = torch.nn.BCEWithLogitsLoss()\n",
    "train_df, val_df, test_df = load_dataset(DATASET_NAME)\n",
    "LABEL_COLS = train_df.columns[1:].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuning implicitly done in score calculation :)\n",
    "def accuracy(y_true, y_pred):\n",
    "    _, best_res = tune_sigmoid_threshold(y_true, y_pred, accuracy_score)\n",
    "    return best_res\n",
    "\n",
    "def jaccard(y_true, y_pred):\n",
    "    _, best_res = tune_sigmoid_threshold(y_true, y_pred, jaccard_score, {'average': 'macro', 'zero_division': 0})\n",
    "    return best_res\n",
    "\n",
    "def jaccard_samples(y_true, y_pred):\n",
    "    _, best_res = tune_sigmoid_threshold(y_true, y_pred, jaccard_score, {'average': 'samples', 'zero_division': 0})\n",
    "    return best_res\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    _, best_res = tune_sigmoid_threshold(y_true, y_pred, f1_score, {'average': 'macro', 'zero_division': 0})\n",
    "    return best_res\n",
    "\n",
    "def f1_micro(y_true, y_pred):\n",
    "    _, best_res = tune_sigmoid_threshold(y_true, y_pred, f1_score, {'average': 'micro', 'zero_division': 0})\n",
    "    return best_res\n",
    "'''\n",
    "weaker accuracy, each prediction is considered correct if its maximum probability class is one of the true classes\n",
    "'''\n",
    "def membership_score(y_true, y_pred):\n",
    "    n_correct = 0\n",
    "    for t_pattern, p_pattern in zip(y_true, y_pred):\n",
    "        n_correct += t_pattern[np.argmax(p_pattern)] == 1\n",
    "    return n_correct / len(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curves(tr_loss, val_loss, score_name = 'loss'):\n",
    "    plt.plot(tr_loss, label='train')\n",
    "    plt.plot(val_loss, label='validation', color='orange', linestyle='--')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel(score_name)\n",
    "    plt.legend()\n",
    "    plt.title(f'{score_name} over epochs')\n",
    "    plt.show()\n",
    "\n",
    "def model_analysis(model, train_df, val_df, target_cols, test_df=None, checkpoint_path=None, checkpoint_score='f1_macro', checkpoint_score_maximize=True):\n",
    "    model.fit(train_df, validation_df=val_df, progress_bar_epoch=True, progress_bar_step=False, checkpoint_path=checkpoint_path, checkpoint_score=checkpoint_score, checkpoint_score_maximize=checkpoint_score_maximize)\n",
    "    # plot learning curves\n",
    "    tr_scores, val_scores = model.get_train_scores(), model.get_val_scores()\n",
    "    tr_loss, val_loss = model.get_train_loss(), model.get_val_loss()\n",
    "    plot_learning_curves(tr_loss, val_loss)\n",
    "    plot_learning_curves(tr_scores['f1_macro'], val_scores['f1_macro'], 'Macro F1')\n",
    "    # get predictions on validation set\n",
    "    out = model.predict(val_df)\n",
    "    target = val_df[target_cols].values\n",
    "    # plot threshold tuning\n",
    "    plot_threshold_tuning(target, out, plot=True)\n",
    "    plot_threshold_tuning(target, out, plot=True, metric_params={'average':'micro', 'zero_division':0}, metric_fun=f1_score, metric_name='F1 Score')\n",
    "    plot_threshold_tuning(target, out, plot=True, metric_params={'average':'macro', 'zero_division':0}, metric_fun=f1_score, metric_name='F1 Score')\n",
    "    # get best threshold\n",
    "    thresh, _ = tune_sigmoid_threshold(target, out, metric_params={'average':'macro', 'zero_division':0}, metric_fun=f1_score)\n",
    "    # plot the confusion matrix for the best threshold\n",
    "    best_out = (out > thresh).astype(int)\n",
    "    plot_multilabel_confusion_heatmap(target, best_out, label_true=target_cols, label_pred=target_cols, normalize=True)\n",
    "    # bar plot over classes\n",
    "    plot_score_barplot(target, best_out, target_cols)\n",
    "    # print classification report\n",
    "    print(classification_report(target, best_out, target_names=target_cols))\n",
    "    # print additional metrics\n",
    "    print('Jaccard Samples Score:', jaccard_score(target, best_out, zero_division=0, average='samples', labels=target_cols))\n",
    "    print('Jaccard Macro Score:', jaccard_score(target, best_out, zero_division=0, average='macro', labels=target_cols))\n",
    "    print('Membership Score:', membership_score(target, out))\n",
    "    if test_df is not None:\n",
    "        # print results on test set using threshold from validation set\n",
    "        # get predictions on test set\n",
    "        out = model.predict(test_df)\n",
    "        target = test_df[target_cols].values\n",
    "        # plot the confusion matrix for the best threshold\n",
    "        best_out = (out > thresh).astype(int)\n",
    "        plot_multilabel_confusion_heatmap(target, best_out, label_true=target_cols, label_pred=target_cols, normalize=True)\n",
    "        # bar plot over classes\n",
    "        plot_score_barplot(target, best_out, target_cols)\n",
    "        # print classification report\n",
    "        print(classification_report(target, best_out, target_names=target_cols))\n",
    "        # print additional metrics\n",
    "        print('Jaccard Samples Score:', jaccard_score(target, best_out, zero_division=0, average='samples', labels=target_cols))\n",
    "        print('Jaccard Macro Score:', jaccard_score(target, best_out, zero_division=0, average='macro', labels=target_cols))\n",
    "        print('Membership Score:', membership_score(target, out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preliminary attempts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model parameters\n",
    "PARAMS = {'batch_size':MINIBATCH_SIZE, \n",
    "          'learning_rate':LEARNING_RATE, \n",
    "          'epochs':EPOCHS, \n",
    "          'loss_function':loss_function, \n",
    "          'regularization':LAMBDA,\n",
    "          'n_classes':len(LABEL_COLS),\n",
    "          'frozen_layers':FROZEN_LAYERS}\n",
    "# scores dictionary\n",
    "SCORES = {'accuracy': accuracy,\n",
    "          'jaccard_macro': jaccard,\n",
    "          'f1_macro': f1,\n",
    "          'jaccard_samples': jaccard_samples,\n",
    "          'f1_micro':f1_micro,\n",
    "          'membership':membership_score}\n",
    "# creating the model\n",
    "model = Roberta(SCORES,\n",
    "                PARAMS)\n",
    "model_analysis(model, train_df, val_df, LABEL_COLS, checkpoint_path=CHECKPOINT_MODEL_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_to_explore = {\n",
    "    'learning_rate': [5e-05, 1e-05],\n",
    "    'regularization': [0, 1e-05],\n",
    "    'batch_size': [16, 32],\n",
    "    'epochs': [6],\n",
    "    'frozen_layers': [FROZEN_LAYERS],\n",
    "    'n_classes': [len(LABEL_COLS)],\n",
    "}\n",
    "RESULT_DIR = './results/' + DATASET_NAME + '/'\n",
    "RESULT_FILE = RESULT_DIR + DATASET_NAME + '_' + MODEL_NAME + '.csv'\n",
    "# create the grid search object\n",
    "grid_search = HoldOutCrossValidation(Roberta, SCORES, train_df, val_df, param_dict=params_to_explore, res_file=RESULT_FILE)\n",
    "# run the grid search\n",
    "grid_search.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the results\n",
    "results = grid_search.get_results()\n",
    "# get the best parameters and scores according to F1 macro\n",
    "best_info = grid_search.get_best_info('f1_macro')\n",
    "print(\"BEST MODEL INFO\")\n",
    "print(best_info)\n",
    "# print the results\n",
    "print(\"RESULTS\")\n",
    "print(results)\n",
    "print(\"BEST PARAMS\")\n",
    "BEST_PARAMS = grid_search.get_best_params('f1_macro')\n",
    "BEST_PARAMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model with the best parameters\n",
    "model = Roberta(SCORES, BEST_PARAMS)\n",
    "model_analysis(model, train_df, val_df, LABEL_COLS, test_df, checkpoint_path=CHECKPOINT_MODEL_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULT_FILE = RESULT_DIR + DATASET_NAME + '_' + MODEL_NAME + '_features.csv'\n",
    "TOP_N = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempt on grouped emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_MODEL_FILE = CHECKPOINT_DIR + DATASET_NAME + '_' + 'Grouped' + '_' + MODEL_NAME + '.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map dataset emotions to twitter\n",
    "train_df = goemotions_apply_emotion_mapping(train_df)\n",
    "val_df = goemotions_apply_emotion_mapping(val_df)\n",
    "test_df = goemotions_apply_emotion_mapping(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model analysis with twitter mapped emotions\n",
    "model = Roberta(SCORES, BEST_PARAMS)\n",
    "model_analysis(model, train_df, val_df, LABEL_COLS, test_df, checkpoint_path=CHECKPOINT_MODEL_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importance on grouped dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULT_FILE = RESULT_DIR + DATASET_NAME + '_' + 'Grouped' + '_' + MODEL_NAME + '_features.csv'\n",
    "TOP_N = 30"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HLT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
