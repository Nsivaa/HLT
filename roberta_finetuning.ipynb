{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries needed\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import seaborn as sns\n",
    "import transformers\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import RobertaModel, RobertaTokenizer\n",
    "import logging\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "from torch import cuda\n",
    "from lib.dataset_utils import *\n",
    "from sklearn.metrics import accuracy_score, jaccard_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Minibatch\n",
    "- learning rate\n",
    "- momentum\n",
    "- regularization\n",
    "- dropout?\n",
    "- topologia\n",
    "- optimizer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO incapsulare iperparametri\n",
    "# Defining some key variables that will be used later on in the training\n",
    "DATASET_NAME = DatasetEnum.GoEmotions\n",
    "MAX_LEN = 256\n",
    "MINIBATCH_SIZE = 8\n",
    "EPOCHS = 1\n",
    "LEARNING_RATE = 1e-05\n",
    "N_CLASSES = 28\n",
    "loader_params = {'batch_size': MINIBATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "# Creating the loss function and optimizer\n",
    "loss_function = torch.nn.BCEWithLogitsLoss()\n",
    "train_df, val_df, test_df = load_dataset(DATASET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO cross validation su topologia\n",
    "class RobertaClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RobertaClass, self).__init__()\n",
    "        self.l1 = RobertaModel.from_pretrained(\"roberta-base\")\n",
    "        self.pre_classifier = torch.nn.Linear(768, 768)\n",
    "        self.dropout = torch.nn.Dropout(0.3)\n",
    "        self.classifier = torch.nn.Linear(768, N_CLASSES)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        hidden_state = output_1[0]\n",
    "        pooler = hidden_state[:, 0]\n",
    "        pooler = self.pre_classifier(pooler)\n",
    "        pooler = torch.nn.ReLU()(pooler)\n",
    "        pooler = self.dropout(pooler)\n",
    "        output = self.classifier(pooler)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_params(optimizer=torch.optim.Adam,\n",
    "                        tokenizer=RobertaTokenizer.from_pretrained('roberta-base', truncation=True, do_lower_case=True),\n",
    "                        tokenizer_max_len=256,\n",
    "                        loader_params={'batch_size': 8, 'shuffle': True, 'num_workers': 0},\n",
    "                        loss_function=torch.nn.BCEWithLogitsLoss(),\n",
    "                        epochs=1,\n",
    "                        learning_rate=1e-05):\n",
    "    return {\n",
    "        'optimizer': optimizer,\n",
    "        'tokenizer': tokenizer,\n",
    "        'tokenizer_max_len': tokenizer_max_len,\n",
    "        'loader_params': loader_params,\n",
    "        'loss_function': loss_function,\n",
    "        'epochs': epochs,\n",
    "        'learning_rate': learning_rate\n",
    "    }\n",
    "\n",
    "class SimpleModelInterface:\n",
    "    def __init__(self, \n",
    "                 model: torch.nn.Module, \n",
    "                 scores={},\n",
    "                 model_params_dict=create_model_params()):\n",
    "        self.model = model\n",
    "        self.params = model_params_dict\n",
    "        self.optimizer = model_params_dict['optimizer'](params=model.parameters(), lr=model_params_dict['learning_rate'])\n",
    "        self.scores = scores\n",
    "        self.train_scores = {name: [] for name in scores.keys()}\n",
    "        self.train_loss = []\n",
    "        self.val_scores = {name: [] for name in scores.keys()}\n",
    "        self.val_loss = []\n",
    "        self.device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "        model.to(self.device)\n",
    "\n",
    "    def _train_epoch(self, training_loader, validation_loader=None):\n",
    "        self.model.train()\n",
    "        #TODO usare confusion matrix?\n",
    "        predictions_acc = torch.tensor([])\n",
    "        targets_acc = torch.tensor([])\n",
    "        tr_loss = 0\n",
    "        for _,data in tqdm(enumerate(training_loader, 0)):\n",
    "            ids = data['ids'].to(self.device, dtype = torch.long)\n",
    "            mask = data['mask'].to(self.device, dtype = torch.long)\n",
    "            token_type_ids = data['token_type_ids'].to(self.device, dtype = torch.long)\n",
    "            targets = data['targets'].to(self.device, dtype = torch.float)\n",
    "\n",
    "            outputs = self.model(ids, mask, token_type_ids)\n",
    "            loss = loss_function(outputs, targets)\n",
    "            tr_loss += loss.item()\n",
    "            # append predictions and targets\n",
    "            predictions_acc = np.append(predictions_acc, torch.sigmoid(outputs).detach().cpu().numpy())\n",
    "            targets_acc = np.append(targets_acc, targets.detach().cpu().numpy())\n",
    "\n",
    "            # backward pass\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "        # calculate training scores\n",
    "        epoch_loss = tr_loss/len(training_loader)\n",
    "        self.train_loss.append(epoch_loss)\n",
    "        for name, score in self.scores.items():\n",
    "            self.train_scores[name].append(score(targets_acc, predictions_acc))\n",
    "\n",
    "        # calculate validation scores\n",
    "        if validation_loader is not None:\n",
    "            val_scores = self.evaluate(validation_loader)\n",
    "            for name, score in val_scores.items():\n",
    "                self.val_scores[name].append(score)\n",
    "            self.val_loss.append(val_scores['loss'])\n",
    "\n",
    "\n",
    "    def fit(self, training_df, validation_df=None):\n",
    "        training_loader = create_data_loader_from_dataframe(training_df, self.params['tokenizer'], self.params['tokenizer_max_len'], **self.params['loader_params'])\n",
    "        validation_loader = None\n",
    "        if validation_df is not None:\n",
    "            validation_loader = create_data_loader_from_dataframe(validation_df, self.params['tokenizer'], self.params['tokenizer_max_len'], **self.params['loader_params'])\n",
    "        for _ in range(self.params['epochs']):\n",
    "            self._train_epoch(training_loader, validation_loader)\n",
    "\n",
    "    def predict(self, data_loader, compute_scores=False):\n",
    "        self.model.eval()\n",
    "        pred_loss=0\n",
    "        predictions_acc = np.array([])\n",
    "        targets_acc = np.array([])\n",
    "        with torch.no_grad():\n",
    "            for _, data in tqdm(enumerate(data_loader, 0)):\n",
    "                ids = data['ids'].to(self.device, dtype = torch.long)\n",
    "                mask = data['mask'].to(self.device, dtype = torch.long)\n",
    "                token_type_ids = data['token_type_ids'].to(self.device, dtype=torch.long)\n",
    "                targets = data['targets'].to(self.device, dtype = torch.float)\n",
    "                outputs = self.model(ids, mask, token_type_ids).squeeze()\n",
    "                if compute_scores:\n",
    "                    # accumulate loss\n",
    "                    loss = loss_function(outputs, targets)\n",
    "                    pred_loss += loss.item()\n",
    "                # append predictions and targets\n",
    "                predictions_acc = np.append(predictions_acc, torch.sigmoid(outputs).detach().cpu().numpy())\n",
    "                targets_acc = np.append(targets_acc, targets.detach().cpu().numpy())\n",
    "        return targets_acc, predictions_acc, pred_loss\n",
    "\n",
    "    def evaluate(self, testing_df, compute_scores=True):\n",
    "        testing_loader = create_data_loader_from_dataframe(testing_df, self.params['tokenizer'], self.params['tokenizer_max_len'], **self.params['loader_params'])\n",
    "        targets_acc, predictions_acc, pred_loss = self.predict(testing_loader, compute_scores)\n",
    "        # calculate scores\n",
    "        scores = {name: score(targets_acc, predictions_acc) for name, score in self.scores.items()}\n",
    "        scores['loss'] = pred_loss/len(testing_loader)\n",
    "        return scores\n",
    "\n",
    "    def get_train_scores(self):\n",
    "        return self.train_scores\n",
    "    \n",
    "    def get_train_loss(self):\n",
    "        return self.train_loss\n",
    "    \n",
    "    def save_model(self, model_path, vocabulary_path):\n",
    "        torch.save(self.model, model_path)\n",
    "        self.tokenizer.save_vocabulary(vocabulary_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD = 0.5\n",
    "def accuracy(y_true, y_pred):\n",
    "    return accuracy_score(y_true, y_pred > THRESHOLD)\n",
    "\n",
    "def jaccard(y_true, y_pred):\n",
    "    return jaccard_score(y_true, y_pred > THRESHOLD)\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    return f1_score(y_true, y_pred > THRESHOLD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = SimpleModelInterface(RobertaClass(), {'accuracy': accuracy, 'jaccard': jaccard, 'f1': f1}, create_model_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125it [00:52,  2.36it/s]\n"
     ]
    }
   ],
   "source": [
    "model.fit(train_df[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125it [00:16,  7.55it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.9586785714285714,\n",
       " 'jaccard': 0.0,\n",
       " 'f1': 0.0,\n",
       " 'loss': 0.21523185777664183}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#FIXME jaccard ed f1 non funzionano correttamente\n",
    "scores = model.evaluate(test_df[:1000])\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HLT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
