{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries needed\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import seaborn as sns\n",
    "import transformers\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import RobertaModel, RobertaTokenizer\n",
    "import logging\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "from torch import cuda\n",
    "from lib.dataset_utils import *\n",
    "from lib.plot_utils import *\n",
    "from lib.models import *\n",
    "from sklearn.metrics import accuracy_score, jaccard_score, f1_score, multilabel_confusion_matrix\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Minibatch\n",
    "- learning rate\n",
    "- momentum\n",
    "- regularization\n",
    "- dropout?\n",
    "- topologia\n",
    "- optimizer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining some key variables that will be used later on in the training\n",
    "DATASET_NAME = DatasetEnum.GoEmotionsCleaned\n",
    "MINIBATCH_SIZE = 16\n",
    "EPOCHS = 1\n",
    "LEARNING_RATE = 5e-05\n",
    "FROZEN_LAYERS = 9\n",
    "# Creating the loss function and optimizer\n",
    "loss_function = torch.nn.BCEWithLogitsLoss()\n",
    "train_df, val_df, test_df = load_dataset(DATASET_NAME)\n",
    "MAX_LEN = compute_max_tokens([train_df, val_df, test_df], RobertaTokenizer.from_pretrained('roberta-base'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD = 0.5\n",
    "# tuning implicitly done in score calculation :)\n",
    "def accuracy(y_true, y_pred):\n",
    "    _, best_res = tune_sigmoid_threshold(y_true, y_pred, accuracy_score)\n",
    "    return best_res\n",
    "\n",
    "def jaccard(y_true, y_pred):\n",
    "    _, best_res = tune_sigmoid_threshold(y_true, y_pred, jaccard_score, {'average': 'macro'})\n",
    "    return best_res\n",
    "\n",
    "def jaccard_samples(y_true, y_pred):\n",
    "    _, best_res = tune_sigmoid_threshold(y_true, y_pred, jaccard_score, {'average': 'samples'})\n",
    "    return best_res\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    _, best_res = tune_sigmoid_threshold(y_true, y_pred, f1_score, {'average': 'macro'})\n",
    "    return best_res\n",
    "\n",
    "def f1_micro(y_true, y_pred):\n",
    "    _, best_res = tune_sigmoid_threshold(y_true, y_pred, f1_score, {'average': 'micro'})\n",
    "    return best_res\n",
    "'''\n",
    "weaker accuracy, each prediction is considered correct it its maximum probability class is one of the true classes\n",
    "'''\n",
    "def membership_score(y_true, y_pred):\n",
    "    n_correct = 0\n",
    "    for t_pattern, p_pattern in zip(y_true, y_pred):\n",
    "        n_correct += t_pattern[np.argmax(p_pattern)] == 1\n",
    "    return n_correct / len(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleModelInterface(RobertaClass(train_df.shape[1]-1, frozen_layers=FROZEN_LAYERS), {'accuracy': accuracy, 'jaccard_macro': jaccard, 'f1_macro': f1, 'jaccard_samples': jaccard_samples, 'f1_micro':f1_micro, 'membership':membership_score}, create_model_params(tokenizer_max_len=MAX_LEN, batch_size=MINIBATCH_SIZE, learning_rate=LEARNING_RATE, epochs=EPOCHS, loss_function=loss_function))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_df[:1000], progress_bar_epoch=True, progress_bar_step=True)#TODO use entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.evaluate(test_df[:1000])\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model.predict(test_df[:1000])\n",
    "target = test_df[:1000].iloc[:, 1:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_threshold_tuning(target, out, plot=True)\n",
    "plot_threshold_tuning(target, out, plot=True, metric_params={'average':'micro'}, metric_fun=f1_score, metric_name='F1 Score')\n",
    "plot_threshold_tuning(target, out, plot=True, metric_params={'average':'macro'}, metric_fun=f1_score, metric_name='F1 Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get best threshold\n",
    "thresh, _ = tune_sigmoid_threshold(target, out, accuracy_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the confusion matrix for the best threshold\n",
    "best_out = (out > thresh).astype(int)\n",
    "plot_multilabel_confusion_heatmap(target, best_out, label_true=test_df.columns[1:], label_pred=test_df.columns[1:], normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bar plot over classes\n",
    "plot_score_barplot(target, best_out, test_df.columns[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO testing code of bootstrap testing\n",
    "# bootstrap testing\n",
    "n_bootstraps = 5\n",
    "n_samples = 100\n",
    "scores = []\n",
    "bootstrap_test(model, model, test_df, n_bootstraps, n_samples, membership_score, 'membership')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HLT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
