{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0482609f-0f7b-483a-952b-980c6497d9d2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Bert fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227937b1-175c-4d3f-8f15-6c9e89adff60",
   "metadata": {},
   "source": [
    "## Go emotions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07bba815-3be1-4537-89b7-d24da9200209",
   "metadata": {},
   "source": [
    "It's a classification task dataset made of 58k reddit comments.\n",
    "Goemotions is a very unique dataset because quite often emotion classification task follows the directive to classify within only 6 emotions (which are very often joy, anger, fear, sadness, disgust, and surprise) proposed by Ekman in 1992 while goemotions classify the entries in 27 emotions + 1 neutral.<br>\n",
    "What follows is based on the official goemotion paper analysis for fine tuning bert."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679dfccd-5eba-4fa9-a7e2-c97d15c2c7b3",
   "metadata": {},
   "source": [
    "### Data preparation\n",
    "The twitter dataset is full of biases and is very toxic so we will take the already prepared data got from the paper work, which provide for example:\n",
    "1. Reducing profanity\n",
    "2. Sentiment balancing\n",
    "3. Subreddit balancing\n",
    "4. Length filtering\n",
    "5. mask proper names referring to people with a [NAME] token and religion terms with a [RELIGION] token.\n",
    "\n",
    "\n",
    "To minimize the noise in our data, we filter out emotion labels selected by only a single annotator. We keep examples with at least one label after this filtering is performed—this amounts to 93% of the original data. We randomly split this data into train (80%), dev (10%) and test (10%) sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "292b79fa-d87c-4598-b5c7-11353fa5f709",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import datetime\n",
    "import gc\n",
    "import random\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from lib.dataset_utils import *\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import seaborn as sns\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler,random_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from lib import dataset_utils\n",
    "import transformers\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig,BertTokenizer,get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84bf9063-eda3-4fe7-ac33-ca5c7264546b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['admiration', 'amusement', 'disapproval', 'disgust', 'embarrassment',\n",
       "       'excitement', 'fear', 'gratitude', 'grief', 'joy', 'love',\n",
       "       'nervousness', 'anger', 'optimism', 'pride', 'realization', 'relief',\n",
       "       'remorse', 'sadness', 'surprise', 'neutral', 'annoyance', 'approval',\n",
       "       'caring', 'confusion', 'curiosity', 'desire', 'disappointment'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df, val_df, test_df = load_dataset(DatasetEnum.GoEmotions, k_hot_encode=True)\n",
    "label_names = train_df.columns[1:]\n",
    "label_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eee4794c-7ece-4ac4-a1b4-840329ac10e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Substitute user tags with [NAME]\n",
    "train_df['text'] = train_df['text'].str.replace(r\"(?: ^|\\b)/?u/\\w+\", '[NAME]', regex=True)\n",
    "val_df['text'] = val_df['text'].str.replace(r\"(?: ^|\\b)/?u/\\w+\", '[NAME]', regex=True)\n",
    "test_df['text'] = test_df['text'].str.replace(r\"(?: ^|\\b)/?u/\\w+\", '[NAME]', regex=True)\n",
    "\n",
    "# Substitute subreddit tags with [LINK]\n",
    "train_df['text'] = train_df['text'].str.replace(r\"(?: ^|\\b)/?r\\/\\w+\", '[LINK]', regex=True)\n",
    "val_df['text'] = val_df['text'].str.replace(r\"(?: ^|\\b)/?r\\/\\w+\", '[LINK]', regex=True)\n",
    "test_df['text'] = test_df['text'].str.replace(r\"(?: ^|\\b)/?r\\/\\w+\", '[LINK]', regex=True)\n",
    "\n",
    "OUT_DIR = \"./dataset/GoEmotionsCleaned/\"\n",
    "\n",
    "# Save the cleaned dataset\n",
    "if not os.path.exists(OUT_DIR):\n",
    "    os.makedirs(OUT_DIR)\n",
    "train_df.to_csv(OUT_DIR + \"train.tsv\", sep='\\t', index=False)\n",
    "val_df.to_csv(OUT_DIR + \"val.tsv\", sep='\\t', index=False)\n",
    "test_df.to_csv(OUT_DIR + \"test.tsv\", sep='\\t', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca925c03-c3f8-4aa2-b36c-a60496eec44b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device=torch.device(\"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c31a20b4-0713-4283-88b1-570a367dd037",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>admiration</th>\n",
       "      <th>amusement</th>\n",
       "      <th>disapproval</th>\n",
       "      <th>disgust</th>\n",
       "      <th>embarrassment</th>\n",
       "      <th>excitement</th>\n",
       "      <th>fear</th>\n",
       "      <th>gratitude</th>\n",
       "      <th>grief</th>\n",
       "      <th>...</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>neutral</th>\n",
       "      <th>annoyance</th>\n",
       "      <th>approval</th>\n",
       "      <th>caring</th>\n",
       "      <th>confusion</th>\n",
       "      <th>curiosity</th>\n",
       "      <th>desire</th>\n",
       "      <th>disappointment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My favourite food is anything I didn't have to...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Now if he does off himself, everyone will thin...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>WHY THE FUCK IS BAYLESS ISOING</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>To make her feel threatened</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dirty Southern Wankers</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43405</th>\n",
       "      <td>Added you mate well I’ve just got the bow and ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43406</th>\n",
       "      <td>Always thought that was funny but is it a refe...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43407</th>\n",
       "      <td>What are you talking about? Anything bad that ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43408</th>\n",
       "      <td>More like a baptism, with sexy results!</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43409</th>\n",
       "      <td>Enjoy the ride!</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>43410 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  admiration  \\\n",
       "0      My favourite food is anything I didn't have to...           0   \n",
       "1      Now if he does off himself, everyone will thin...           0   \n",
       "2                         WHY THE FUCK IS BAYLESS ISOING           0   \n",
       "3                            To make her feel threatened           0   \n",
       "4                                 Dirty Southern Wankers           0   \n",
       "...                                                  ...         ...   \n",
       "43405  Added you mate well I’ve just got the bow and ...           0   \n",
       "43406  Always thought that was funny but is it a refe...           0   \n",
       "43407  What are you talking about? Anything bad that ...           0   \n",
       "43408            More like a baptism, with sexy results!           0   \n",
       "43409                                    Enjoy the ride!           0   \n",
       "\n",
       "       amusement  disapproval  disgust  embarrassment  excitement  fear  \\\n",
       "0              0            0        0              0           0     0   \n",
       "1              0            0        0              0           0     0   \n",
       "2              0            0        0              0           0     0   \n",
       "3              0            0        0              0           0     1   \n",
       "4              0            0        0              0           0     0   \n",
       "...          ...          ...      ...            ...         ...   ...   \n",
       "43405          0            0        0              0           0     0   \n",
       "43406          0            0        0              0           0     0   \n",
       "43407          0            0        0              0           0     0   \n",
       "43408          0            0        0              0           1     0   \n",
       "43409          0            0        0              0           0     0   \n",
       "\n",
       "       gratitude  grief  ...  sadness  surprise  neutral  annoyance  approval  \\\n",
       "0              0      0  ...        0         0        1          0         0   \n",
       "1              0      0  ...        0         0        1          0         0   \n",
       "2              0      0  ...        0         0        0          0         0   \n",
       "3              0      0  ...        0         0        0          0         0   \n",
       "4              0      0  ...        0         0        0          1         0   \n",
       "...          ...    ...  ...      ...       ...      ...        ...       ...   \n",
       "43405          0      0  ...        0         0        0          0         0   \n",
       "43406          0      0  ...        0         0        0          0         0   \n",
       "43407          0      0  ...        0         0        0          1         0   \n",
       "43408          0      0  ...        0         0        0          0         0   \n",
       "43409          0      0  ...        0         0        0          0         0   \n",
       "\n",
       "       caring  confusion  curiosity  desire  disappointment  \n",
       "0           0          0          0       0               0  \n",
       "1           0          0          0       0               0  \n",
       "2           0          0          0       0               0  \n",
       "3           0          0          0       0               0  \n",
       "4           0          0          0       0               0  \n",
       "...       ...        ...        ...     ...             ...  \n",
       "43405       0          0          0       0               0  \n",
       "43406       0          1          0       0               0  \n",
       "43407       0          0          0       0               0  \n",
       "43408       0          0          0       0               0  \n",
       "43409       0          0          0       0               0  \n",
       "\n",
       "[43410 rows x 29 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a24def53-3173-4bb2-924a-de0e3e031e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We utilize the uncased version of Bert where both HELLO and Hello will be tokenized as hello\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "544215d4-a474-4091-b14a-cda017011879",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=train_df\n",
    "tr_text=np.asarray(df[\"text\"])\n",
    "#prima c'era un as_array\n",
    "tr_labels =df.drop(columns=[\"text\"])\n",
    "tr_labels=torch.tensor(tr_labels.values, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f50b805-d2ea-44cd-bd3a-94681d7a6de2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([43410, 28])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5dbce9ca-1434-46b4-88f9-51933dd45a8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Original:  My favourite food is anything I didn't have to cook myself.\n",
      "Tokenized:  ['my', 'favourite', 'food', 'is', 'anything', 'i', 'didn', \"'\", 't', 'have', 'to', 'cook', 'myself', '.']\n",
      "Token IDs:  [2026, 8837, 2833, 2003, 2505, 1045, 2134, 1005, 1056, 2031, 2000, 5660, 2870, 1012]\n"
     ]
    }
   ],
   "source": [
    "print(' Original: ', tr_text[0])\n",
    "\n",
    "# Print the sentence split into tokens.\n",
    "print('Tokenized: ', tokenizer.tokenize(tr_text[0]))\n",
    "\n",
    "# Print the sentence mapped to token ids.\n",
    "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(tr_text[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d22ce3f0-a8ec-4699-b7de-fd8a9b716259",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 47\n",
    "temp=[]\n",
    "tempL =[]\n",
    "# For every sentence...\n",
    "#rimuoviamo tutte le entry di lunghezza maggiore di 47 => outliers\n",
    "for i in range(len(tr_text)):\n",
    "\n",
    "    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
    "    input_ids = tokenizer.encode(tr_text[i], add_special_tokens=True)\n",
    "    if len(input_ids) <= max_len:\n",
    "        temp.append(tr_text[i])\n",
    "        tempL.append(tr_labels[i])\n",
    "        \n",
    "tr_text=np.asarray(temp)\n",
    "tr_labels=torch.stack(tempL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e953632a-a310-40ef-834d-f0f337b19fa1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lengths' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m unique_values, counts \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths, return_counts\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(unique_values)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(counts)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'lengths' is not defined"
     ]
    }
   ],
   "source": [
    "unique_values, counts = np.unique(lengths, return_counts=True)\n",
    "print(unique_values)\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "14114ef0-f882-4de5-82e5-dabbaef5acc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "C:\\Users\\gdema\\anaconda3\\envs\\Hlt\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2688: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  My favourite food is anything I didn't have to cook myself.\n",
      "Token IDs: tensor([ 101, 2026, 8837, 2833, 2003, 2505, 1045, 2134, 1005, 1056, 2031, 2000,\n",
      "        5660, 2870, 1012,  102,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0])\n"
     ]
    }
   ],
   "source": [
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "# For every tweet...\n",
    "for text in tr_text:\n",
    "    # `encode_plus` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    #   (5) Pad or truncate the sentence to `max_length`\n",
    "    #   (6) Create attention masks for [PAD] tokens.\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        text,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 47,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "    \n",
    "    # Add the encoded sentence to the list.    \n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    \n",
    "    # And its attention mask (simply differentiates padding from non-padding).\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "# Convert the lists into tensors.\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "\n",
    "# Print sentence 0, now as a list of IDs.\n",
    "print('Original: ', tr_text[0])\n",
    "print('Token IDs:', input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fc6964ba-9bf5-461c-ae46-1bacc053f614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43,399 training samples\n"
     ]
    }
   ],
   "source": [
    "# Combine the training inputs into a TensorDataset.\n",
    "train_dataset = TensorDataset(input_ids, attention_masks, tr_labels)\n",
    "\n",
    "print('{:>5,} training samples'.format(len(train_dataset)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5416b068-844f-4730-baa8-d251885d7074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The DataLoader needs to know our batch size for training, so we specify it \n",
    "# here. For fine-tuning BERT on a specific task, the authors recommend a batch \n",
    "# size of 16 or 32.\n",
    "# 16 for the paper\n",
    "batch_size = 16\n",
    "\n",
    "# Create the DataLoaders for our training and validation sets.\n",
    "# We'll take training samples in random order. \n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,  # The training samples.\n",
    "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
    "            batch_size = batch_size # Trains with this batch size.\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "80ea338e-5152-4ef1-bc52-a787b53b6f53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#overridiamo la classe di bert per modificare la funzione di loss e altri hyperparametri del modello\\nfrom transformers import BertModel\\nfrom transformers.modeling_outputs import SequenceClassifierOutput\\nimport torch.nn as nn\\n\\nclass CustomBert(nn.Module):\\n    def __init__(self):\\n        super(CustomBert, self).__init__()\\n        self.bert = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\")\\n        self.dropout = nn.Dropout(0.1)\\n        self.output = nn.Linear(768, 28)\\n\\n    def forward(self, ids, attention_mask, token_type_ids, labels=None):\\n        outputs = self.bert(ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\\n        output = self.dropout(outputs)\\n        logits = self.output(output)\\n    \\n        loss = None\\n        if labels is not None:\\n            \\n            loss_fct = nn.BCEWithLogitsLoss()\\n            # next, compute the loss based on logits + ground-truth labels\\n            # TODO: debuggare se i tensori input alla loss sono corretti di dimensione\\n            loss = loss_fct(logits, labels)\\n\\n        return SequenceClassifierOutput(\\n            loss=loss,\\n            logits=logits)\\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "#overridiamo la classe di bert per modificare la funzione di loss e altri hyperparametri del modello\n",
    "from transformers import BertModel\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "import torch.nn as nn\n",
    "\n",
    "class CustomBert(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomBert, self).__init__()\n",
    "        self.bert = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.output = nn.Linear(768, 28)\n",
    "\n",
    "    def forward(self, ids, attention_mask, token_type_ids, labels=None):\n",
    "        outputs = self.bert(ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        output = self.dropout(outputs)\n",
    "        logits = self.output(output)\n",
    "    \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            \n",
    "            loss_fct = nn.BCEWithLogitsLoss()\n",
    "            # next, compute the loss based on logits + ground-truth labels\n",
    "            # TODO: debuggare se i tensori input alla loss sono corretti di dimensione\n",
    "            loss = loss_fct(logits, labels)\n",
    "\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "96c15697-f147-446a-8284-de2f68fbd7ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
    "# linear classification layer on top. \n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",num_labels=28, problem_type=\"multi_label_classification\",hidden_dropout_prob=0.1)\n",
    "\n",
    "# if device == \"cuda:0\":\n",
    "# # Tell pytorch to run this model on the GPU.\n",
    "#     model = model.cuda()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "185439c1-5615-4ba1-b570-4ed5878e5206",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(),\n",
    "                  lr = 5e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b00f83-cb35-4038-a631-c6d9d5387bd0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0b20aaa4-799d-41b2-aad5-7feb713d1add",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of training epochs. The BERT authors recommend between 2 and 4. \n",
    "# We chose to run for 4, but we'll see later that this may be over-fitting the\n",
    "# training data.\n",
    "epochs = 6\n",
    "\n",
    "# Total number of training steps is [number of batches] x [number of epochs]. \n",
    "# (Note that this is not the same as the number of training samples).\n",
    "total_steps = len(train_dataloader) * epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "79011c32-33c8-4814-9185-b60287995ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1fa5be18-574c-44eb-ad64-2586459324f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "056d5f04-5d7c-405f-b6d1-658094f1d039",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_val = 42\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "05614b9d-4853-427c-bd30-6a86fb2256a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e948dbb3-75cd-43c3-99b0-ea3b3edb9a38",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 6 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.11\n",
      "  Training epoch took: 0:11:38\n",
      "\n",
      "======== Epoch 2 / 6 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.08\n",
      "  Training epoch took: 0:11:37\n",
      "\n",
      "======== Epoch 3 / 6 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.07\n",
      "  Training epoch took: 0:11:37\n",
      "\n",
      "======== Epoch 4 / 6 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.05\n",
      "  Training epoch took: 0:11:38\n",
      "\n",
      "======== Epoch 5 / 6 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.04\n",
      "  Training epoch took: 0:11:38\n",
      "\n",
      "======== Epoch 6 / 6 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.03\n",
      "  Training epoch took: 0:11:38\n",
      "\n",
      "Training complete!\n",
      "Total training took 1:09:50 (h:mm:ss)\n"
     ]
    }
   ],
   "source": [
    "loss=0\n",
    "training_stats = []\n",
    "import gc\n",
    "\n",
    "# Measure the total training time for the whole run.\n",
    "total_t0 = time.time()\n",
    "\n",
    "# For each epoch...\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    # Perform one full pass over the training set.\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "    # Measure how long the training epoch takes.\n",
    "    t0 = time.time()\n",
    "    total_train_loss = 0\n",
    "    model.train()\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # Unpack this training batch from our dataloader. \n",
    "        #\n",
    "        # As we unpack the batch, we'll also copy each tensor to the device using the \n",
    "        # `to` method.\n",
    "        #\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(b_input_ids, \n",
    "                             token_type_ids=None, \n",
    "                             attention_mask=b_input_mask, \n",
    "                             labels=b_labels)\n",
    "        del b_input_ids\n",
    "        del b_input_mask\n",
    "        del b_labels\n",
    "        gc.collect()\n",
    "        loss = output.loss\n",
    "        total_train_loss += loss.item()\n",
    "        # Perform a backward pass to calculate the gradients.\n",
    "        loss.backward()\n",
    "        # Clip the norm of the gradients to 1.0.\n",
    "        # This is to help prevent the \"exploding gradients\" problem.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        # Update parameters and take a step using the computed gradient.\n",
    "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
    "        # modified based on their gradients, the learning rate, etc.\n",
    "        optimizer.step()\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
    "    \n",
    "    # Measure how long this epoch took.\n",
    "    training_time = format_time(time.time() - t0)\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epoch took: {:}\".format(training_time))\n",
    "    torch.save(model, 'bert-emotionsClassification'+str(epoch_i))\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "\n",
    "\n",
    "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "4a33df5c-c917-4f77-8ef6-f983f44c83fb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#loading the best model\n",
    "model = torch.load('bert-emotionsClassification5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b6d05f-2e2d-47e1-a9aa-e959ac08d07e",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "7182838b-5232-4ffe-80fa-38e00b313829",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_text = np.asarray(val_df[\"text\"])\n",
    "val_labels =val_df.drop(columns=[\"text\"])\n",
    "val_labels=torch.tensor(val_labels.values, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fdd924-28d8-4812-9c4c-8e4ac6599a83",
   "metadata": {},
   "source": [
    "Dato che stiamo facendo test non ha senso rimuovere entry con lunghezza maggiore di 47, si fa padding alla lunghezza dell'entry di lunghezza massima del test set (rispettando i limiti di BERT ( 512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "9efdb838-03c1-4e08-9b85-b9fe0e85375b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sentence length:  46\n"
     ]
    }
   ],
   "source": [
    "max_len = 0\n",
    "\n",
    "# For every sentence...\n",
    "for text in val_text:\n",
    "\n",
    "    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
    "    val_ids = tokenizer.encode(text, add_special_tokens=True)\n",
    "\n",
    "    # Update the maximum sentence length.\n",
    "    max_len = max(max_len, len(val_ids))\n",
    "\n",
    "print('Max sentence length: ', max_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "92d3d0cc-a08f-45a6-adfe-abe7c0a92edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gdema\\anaconda3\\envs\\Hlt\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2688: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "C:\\Users\\gdema\\AppData\\Local\\Temp\\ipykernel_13120\\3735325732.py:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  val_labels = torch.tensor(val_labels,dtype=torch.long)\n"
     ]
    }
   ],
   "source": [
    "val_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "# For every tweet...\n",
    "for text in val_text:\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        text,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = max_len,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "    \n",
    "    # Add the encoded sentence to the list.    \n",
    "    val_ids.append(encoded_dict['input_ids'])\n",
    "    \n",
    "    # And its attention mask (simply differentiates padding from non-padding).\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "# Convert the lists into tensors.\n",
    "val_ids = torch.cat(val_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "val_labels = torch.tensor(val_labels,dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "516264b8-89b9-47d5-9ab6-0cdb149db33e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5,426 training samples\n"
     ]
    }
   ],
   "source": [
    "# Combine the training inputs into a TensorDataset.\n",
    "val_dataset = TensorDataset(val_ids, attention_masks)\n",
    "\n",
    "print('{:>5,} training samples'.format(len(val_dataset)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "9c35a3a2-4fd1-4aba-b812-6efaac2110ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The DataLoader needs to know our batch size for training, so we specify it \n",
    "# here. For fine-tuning BERT on a specific task, the authors recommend a batch \n",
    "# size of 16 or 32.\n",
    "# 16 for the paper\n",
    "batch_size = 64\n",
    "\n",
    "# Create the DataLoaders for our training and validation sets.\n",
    "# We'll take training samples in random order. \n",
    "val_dataloader = DataLoader(\n",
    "            val_dataset,  # The training samples.\n",
    "            sampler = RandomSampler(val_dataset), # Select batches randomly\n",
    "            batch_size = batch_size # Trains with this batch size.\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "15effa48-4373-4cef-88ad-b07c10636277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 46])\n"
     ]
    }
   ],
   "source": [
    "for batch in val_dataloader:\n",
    "    print(batch[0].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "80290f0a-5f02-4c37-ae87-db3c5e532f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "model.eval()\n",
    "predictions = []\n",
    "for batch in val_dataloader:\n",
    "    \n",
    "    b_input_ids = batch[0].to(device)\n",
    "    b_input_mask = batch[1].to(device)\n",
    "    with torch.no_grad():        \n",
    "        output= model(b_input_ids, \n",
    "                               token_type_ids=None, \n",
    "                               attention_mask=b_input_mask)\n",
    "        logits = output.logits\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        predictions.extend(logits)\n",
    "\n",
    "predictions=np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "c0e60abf-c9b3-4e1e-a310-1152316ac8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# si applica la sugmoide per ttenere probabilità\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "predictions=sigmoid(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "009cbe4b-c1b9-4a34-a559-fa5310206625",
   "metadata": {},
   "outputs": [],
   "source": [
    "#verifichiamo con cross validation per f1-score il miglior threshold per determinare quando viene classificata o meno un emozione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "2365207f-3f87-49d6-883e-08ada2b85612",
   "metadata": {},
   "outputs": [],
   "source": [
    "def applyThreshold(x,threshold):\n",
    "    if x>= threshold:\n",
    "        return 1\n",
    "    return 0\n",
    "from sklearn.metrics import f1_score\n",
    "results=[]\n",
    "for threshold in np.arange(0.001,0.1,0.001):\n",
    "    newPred=np.where( predictions>= threshold, 1,0)\n",
    "    results.append( f1_score(val_labels,newPred,average=\"weighted\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "c16ff550-1090-4d05-9091-72ec4e125339",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1b5daecbc50>]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.arange(0.001,0.1,0.001),results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "bec52d6d-eed4-4400-b4e9-88e6c1980c82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.11415168, 0.06418485, 0.08142999, 0.00694444, 0.        ,\n",
       "       0.01084011, 0.01904762, 0.05875153, 0.        , 0.03442879,\n",
       "       0.08183306, 0.03846154, 0.04900459, 0.0622084 , 0.        ,\n",
       "       0.02903226, 0.01204819, 0.01142857, 0.05116279, 0.03333333,\n",
       "       0.44510386, 0.07946027, 0.10975012, 0.03558719, 0.05272408,\n",
       "       0.06342016, 0.025     , 0.03921569])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newPred=np.where( predictions>= 0.03, 1,0)\n",
    "f1_score(val_labels,newPred,average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "ae9e5b84-c163-45ef-8181-5e5a0c0aaa93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newPred[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "c5522617-e19c-4daf-bd95-fd6d8ea3b580",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4130.,  2328.,  2021.,   793.,   303.,   852.,   596.,  2662.,    77.,\n",
       "         1451.,  2085.,   164.,  1567.,  1581.,   111.,  1110.,   153.,   545.,\n",
       "         1326.,  1060., 14216.,  2469.,  2939.,  1087.,  1366.,  2189.,   641.,\n",
       "         1269.])"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_labels.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4705b60e-ebd2-45b3-b281-51e95e735083",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hlt",
   "language": "python",
   "name": "hlt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
