{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.dataset_utils import *\n",
    "import pickle\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from lib.scores import *\n",
    "from lib.plot_utils import *\n",
    "from lib.models import bootstrap_test, Bert, Roberta\n",
    "from sklearn.metrics import f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS = [\"Bayes\", \"DecisionTree\", \"RandomForest\", \"Bert\", \"Roberta\", \"SocBert\", \"Llama3 Zero Shot\", \"Llama3 Three Shot\"]\n",
    "DATASETS = [\"GoEmotions\", \"TwitterData\", \"GoEmotionsGrouped\"]\n",
    "BASE_MODELS_DIR = \"./checkpoints/\"\n",
    "\n",
    "GOEMOTIONS_MODELS_PATH = {\n",
    "    \"Bayes\": f\"{BASE_MODELS_DIR}GoEmotions/nb_classifier.pkl\",\n",
    "    \"DecisionTree\": f\"{BASE_MODELS_DIR}GoEmotions/dt_classifier.pkl\",\n",
    "    \"RandomForest\": f\"{BASE_MODELS_DIR}GoEmotions/rf_classifier.pkl\",\n",
    "    \"Bert\": f\"{BASE_MODELS_DIR}GoEmotions/GoEmotions_Bert.pth\",\n",
    "    \"Roberta\": f\"{BASE_MODELS_DIR}GoEmotions/GoEmotions_Roberta.pth\",\n",
    "    \"SocBert\": f\"{BASE_MODELS_DIR}GoEmotions/GoEmotions_Socbert.pth\",\n",
    "}\n",
    "\n",
    "GOEMOTIONS_GROUPED_MODELS_PATH = {\n",
    "    \"Bayes\": f\"{BASE_MODELS_DIR}GoEmotions/GoEmotions_grouped_bayes.pkl\",\n",
    "    \"DecisionTree\": f\"{BASE_MODELS_DIR}GoEmotions/GoEmotions_grouped_dt.pkl\",\n",
    "    \"RandomForest\": f\"{BASE_MODELS_DIR}GoEmotions/GoEmotions_grouped_rt.pkl\",\n",
    "    \"Bert\": f\"{BASE_MODELS_DIR}GoEmotions/GoEmotions_Bert_Ekman.pth\",\n",
    "    \"Roberta\": f\"{BASE_MODELS_DIR}GoEmotions/GoEmotions_Grouped_Roberta.pth\",\n",
    "    \"SocBert\": f\"{BASE_MODELS_DIR}GoEmotions/GoEmotions_Socbert_Ekman.pth\",\n",
    "}\n",
    "\n",
    "TWITTER_MODELS_PATH = {\n",
    "    \"Bayes\": f\"{BASE_MODELS_DIR}TwitterData/nb_classifier.pkl\",\n",
    "    \"DecisionTree\": f\"{BASE_MODELS_DIR}TwitterData/dt_classifier.pkl\",\n",
    "    \"RandomForest\": f\"{BASE_MODELS_DIR}TwitterData/rf_classifier.pkl\",\n",
    "    \"Bert\": f\"{BASE_MODELS_DIR}TwitterData/TwitterData_Bert.pth\",\n",
    "    \"Roberta\": f\"{BASE_MODELS_DIR}TwitterData/TwitterData_Roberta.pth\",\n",
    "    \"SocBert\": f\"{BASE_MODELS_DIR}TwitterData/TwitterData_Socbert.pth\",\n",
    "}\n",
    "\n",
    "DATASET_TO_PATH_DICT = {\n",
    "    \"GoEmotions\": GOEMOTIONS_MODELS_PATH,\n",
    "    \"TwitterData\": TWITTER_MODELS_PATH,\n",
    "    \"GoEmotionsGrouped\": GOEMOTIONS_GROUPED_MODELS_PATH\n",
    "}\n",
    "\n",
    "DATASET_N_LABELS = {\n",
    "    \"GoEmotions\": 28,\n",
    "    \"TwitterData\": 6,\n",
    "    \"GoEmotionsGrouped\": 6\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_goemotions(cleaning=True):\n",
    "    dataset = DatasetEnum.GoEmotionsCleaned if cleaning else DatasetEnum.GoEmotions\n",
    "    if cleaning:\n",
    "        return load_dataset(dataset)\n",
    "    else:\n",
    "        return load_dataset(dataset, k_hot_encode=True)\n",
    "\n",
    "def get_twitterdata(cleaning=True):\n",
    "    dataset = DatasetEnum.TwitterDataCleaned if cleaning else DatasetEnum.TwitterData\n",
    "    if cleaning:\n",
    "        return load_dataset(dataset)\n",
    "    else:\n",
    "        return load_dataset(dataset, k_hot_encode=True)\n",
    "\n",
    "def get_goemotions_grouped(cleaning=True):\n",
    "    train_df, val_df, test_df = get_goemotions(cleaning)\n",
    "    # group emotions\n",
    "    train_df = goemotions_apply_emotion_mapping(train_df)\n",
    "    val_df = goemotions_apply_emotion_mapping(val_df)\n",
    "    test_df = goemotions_apply_emotion_mapping(test_df)\n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "DATASET_LOADERS = {\n",
    "    \"GoEmotions\": get_goemotions,\n",
    "    \"TwitterData\": get_twitterdata,\n",
    "    \"GoEmotionsGrouped\": get_goemotions_grouped\n",
    "}\n",
    "\n",
    "REQUIRES_CLEANING = {\n",
    "    \"Bayes\": True,\n",
    "    \"DecisionTree\": True,\n",
    "    \"RandomForest\": True,\n",
    "    \"Bert\": False,\n",
    "    \"Roberta\": False,\n",
    "    \"SocBert\": False,\n",
    "    \"Llama3 Zero Shot\": True,\n",
    "    \"Llama3 Three Shot\": True\n",
    "}\n",
    "\n",
    "def get_dataset(dataset, cleaning=True):\n",
    "    return DATASET_LOADERS[dataset](cleaning)\n",
    "\n",
    "def bayes_predict(dataset):\n",
    "    # file path\n",
    "    model_path = DATASET_TO_PATH_DICT[dataset][\"Bayes\"]\n",
    "    # load model\n",
    "    with open(model_path, 'rb') as f:\n",
    "        model = pickle.load(f)\n",
    "    # load test data\n",
    "    _, _, test_df = get_dataset(dataset)\n",
    "    # predict\n",
    "    predictions = model.predict(test_df['text'])\n",
    "    # transform to dataframe\n",
    "    predictions_df = pd.DataFrame(predictions, columns=test_df.columns[1:])\n",
    "    return predictions_df\n",
    "\n",
    "def clean_content(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    # tokenize\n",
    "    clean_text = nltk.word_tokenize(text)\n",
    "    # pos tag\n",
    "    clean_text = nltk.pos_tag(clean_text)\n",
    "    TAG_MAP = {'N': 'n', 'V': 'v', 'R': 'r', 'J': 'a'}\n",
    "    clean_text = [(word, TAG_MAP.get(tag[0], 'n')) for word, tag in clean_text]\n",
    "    # lemmatize\n",
    "    clean_text = [lemmatizer.lemmatize(word, tag) for word, tag in clean_text]\n",
    "    # remove punctuation marks\n",
    "    clean_text = [w for w in clean_text if w not in string.punctuation]\n",
    "    return ' '.join(clean_text)\n",
    "\n",
    "def clean_df(df, text_col, out_col):\n",
    "    df[out_col] = df[text_col].apply(clean_content)\n",
    "    return df\n",
    "\n",
    "def decision_tree_predict(dataset):\n",
    "    # file path\n",
    "    model_path = DATASET_TO_PATH_DICT[dataset][\"DecisionTree\"]\n",
    "    # load model\n",
    "    with open(model_path, 'rb') as f:\n",
    "        model = pickle.load(f)\n",
    "    # load test data\n",
    "    _, _, test_df = get_dataset(dataset)\n",
    "    # apply cleaning\n",
    "    predict_df = clean_df(test_df, 'text', 'text')\n",
    "    # predict\n",
    "    predictions = model.predict(predict_df['text'])\n",
    "    # transform to dataframe\n",
    "    predictions_df = pd.DataFrame(predictions, columns=predict_df.columns[1:])\n",
    "    return predictions_df\n",
    "\n",
    "def random_forest_predict(dataset):\n",
    "    # file path\n",
    "    model_path = DATASET_TO_PATH_DICT[dataset][\"RandomForest\"]\n",
    "    # load model\n",
    "    with open(model_path, 'rb') as f:\n",
    "        model = pickle.load(f)\n",
    "    # load test data\n",
    "    _, _, test_df = get_dataset(dataset)\n",
    "    # apply cleaning\n",
    "    predict_df = clean_df(test_df, 'text', 'text')\n",
    "    # predict\n",
    "    predictions = model.predict(predict_df['text'])\n",
    "    # transform to dataframe\n",
    "    predictions_df = pd.DataFrame(predictions, columns=predict_df.columns[1:])\n",
    "    return predictions_df\n",
    "\n",
    "def transformer_predict(dataset, model_name):\n",
    "    # file path\n",
    "    model_path = DATASET_TO_PATH_DICT[dataset][model_name]\n",
    "    # init basic parameters\n",
    "    params = {\n",
    "        \"n_classes\": DATASET_N_LABELS[dataset],\n",
    "    }\n",
    "    # load model\n",
    "    model = Bert(checkpoint=model_path, model_params_dict=params)\n",
    "    # load test data\n",
    "    _, val_df, test_df = get_dataset(dataset, cleaning=False)\n",
    "    # predict on test data\n",
    "    predictions = model.predict(test_df)\n",
    "    if dataset != \"TwitterData\":\n",
    "        # predict on validation data\n",
    "        predictions = model.predict(val_df)\n",
    "        thresh, _ = tune_sigmoid_threshold(val_df[val_df.columns[1:]], predictions, f1_score, metric_params={\"average\": \"macro\", \"zero_division\": 0})\n",
    "        # apply threshold\n",
    "        predictions = (predictions > thresh).astype(int)\n",
    "    else:\n",
    "        predictions = np.argmax(predictions, axis=1)\n",
    "    # transform to dataframe\n",
    "    predictions_df = pd.DataFrame(predictions, columns=test_df.columns[1:])\n",
    "    return predictions_df\n",
    "\n",
    "def bert_predict(dataset):\n",
    "    return transformer_predict(dataset, \"Bert\")\n",
    "\n",
    "def socbert_predict(dataset):\n",
    "    return transformer_predict(dataset, \"SocBert\")\n",
    "\n",
    "def roberta_predict(dataset):\n",
    "    return transformer_predict(dataset, \"Roberta\")\n",
    "\n",
    "LLAMA_ZERO_CSV = {\n",
    "    \"GoEmotions\": \"./results/llama_predictions/llama_multi_0_predictions.csv\",\n",
    "    \"TwitterData\": \"./results/llama_predictions/llama_single_0_predictions.csv\",\n",
    "    \"GoEmotionsGrouped\": './results/llama_predictions/llama_grouped_0_predictions.csv'\n",
    "}\n",
    "\n",
    "LLAMA_THREE_CSV = {\n",
    "    \"GoEmotions\": \"./results/llama_predictions/llama_multi_396_predictions.csv\",\n",
    "    \"TwitterData\": \"./results/llama_predictions/llama_single_355_predictions.csv\",\n",
    "    \"GoEmotionsGrouped\": './results/llama_predictions/llama_grouped_377_predictions.csv'\n",
    "}\n",
    "\n",
    "def llama3_three_predict(dataset):\n",
    "    # load predictions csv\n",
    "    to_read = LLAMA_THREE_CSV[dataset]\n",
    "    if to_read is None:\n",
    "        return None\n",
    "    return pd.read_csv(to_read, index_col=0)\n",
    "\n",
    "def llama3_zero_predict(dataset):\n",
    "    # load predictions csv\n",
    "    to_read = LLAMA_ZERO_CSV[dataset]\n",
    "    if to_read is None:\n",
    "        return None\n",
    "    return pd.read_csv(to_read, index_col=0)\n",
    "\n",
    "PREDICTOR_DICT = {\n",
    "    \"Bayes\": bayes_predict,\n",
    "    \"DecisionTree\": decision_tree_predict,\n",
    "    \"RandomForest\": random_forest_predict,\n",
    "    \"Bert\": bert_predict,\n",
    "    \"Roberta\": roberta_predict,\n",
    "    \"SocBert\": socbert_predict,\n",
    "    \"Llama3 Zero Shot\": llama3_zero_predict,\n",
    "    \"Llama3 Three Shot\": llama3_three_predict\n",
    "}\n",
    "\n",
    "def predict(model, dataset):\n",
    "    return PREDICTOR_DICT[model](dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_scores(model, dataset):\n",
    "    predictions = predict(model, dataset)\n",
    "    _, _, test_df = get_dataset(dataset, cleaning=REQUIRES_CLEANING[model])\n",
    "    labels_list = test_df.columns[1:]\n",
    "    return get_scores_dict(predictions, test_df, labels_list)\n",
    "\n",
    "def print_results(model_name, dataset_name, predictions_df, targets_df):\n",
    "    print(\"----------------------------------------------------------------------\")\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"Dataset: {dataset_name}\")\n",
    "    plot_multilabel_confusion_heatmap(targets_df.values, predictions_df.values, label_true=targets_df.columns, label_pred=targets_df.columns, normalize=True)\n",
    "    # bar plot over classes\n",
    "    plot_score_barplot(targets_df.values, predictions_df.values, targets_df.columns)\n",
    "    # compute scores\n",
    "    scores = get_model_scores(model_name, dataset_name)\n",
    "    # print scores\n",
    "    custom_classification_report(scores, targets_df.columns)\n",
    "    return scores\n",
    "\n",
    "def comparison_bar_plot(scores_dict):\n",
    "    for dataset in DATASETS:\n",
    "        print_dict = {}\n",
    "        for score_id, score_name in zip(['f1-score', 'jaccard'], ['F1 Score', 'Jaccard']):\n",
    "            for model in scores_dict[dataset].keys():\n",
    "                print_dict[model] = scores_dict[dataset][model]['macro avg'][score_id]\n",
    "            # create bar plot with model labels rotated by 90 degrees\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            sns.barplot(x=list(print_dict.keys()), y=list(print_dict.values()), palette=sns.color_palette(\"hls\", len(print_dict)))\n",
    "            plt.xticks(rotation=90)\n",
    "            plt.title(f\"Macro {score_name} for {dataset}\")\n",
    "            plt.xlabel(\"Model\")\n",
    "            plt.ylabel(f\"Macro {score_name}\")\n",
    "            plt.show()\n",
    "\n",
    "def print_dataset_results(dataset):\n",
    "    scores = {}\n",
    "    for model in MODELS:\n",
    "        predictions_df = predict(model, dataset)\n",
    "        if predictions_df is None:\n",
    "            continue\n",
    "        _, _, targets_df = get_dataset(dataset, cleaning=REQUIRES_CLEANING[model])\n",
    "        # sort columns to match\n",
    "        predictions_df = predictions_df[targets_df.columns[1:]]\n",
    "        cur_scores = print_results(model, dataset, predictions_df, targets_df[targets_df.columns[1:]])\n",
    "        scores[model] = cur_scores\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance comparison on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print results for all models on all datasets\n",
    "all_scores = {}\n",
    "for dataset in DATASETS:\n",
    "    all_scores[dataset] = print_dataset_results(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_bar_plot(all_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TESTS = 100\n",
    "SAMPLE_SIZE = 200\n",
    "# get best model for each dataset\n",
    "best_models = {}\n",
    "for dataset in DATASETS:\n",
    "    best_model = max(all_scores[dataset].items(), key=lambda x: x[1]['macro avg']['f1-score'])[0]\n",
    "    best_models[dataset] = best_model\n",
    "    # get labels\n",
    "    labels = get_dataset(dataset)[2]\n",
    "    labels = labels[labels.columns[1:]]\n",
    "    # get best model predictions\n",
    "    best_predictions = predict(best_model, dataset)\n",
    "    # compare best model with all other models according to bootstrap test\n",
    "    for model in MODELS:\n",
    "        if model != best_model:\n",
    "            # get predictions\n",
    "            predictions = predict(model, dataset)\n",
    "            if predictions is None:\n",
    "                continue\n",
    "            print(f\"Bootstrap test between {best_model} and {model} on {dataset}\")\n",
    "            bootstrap_test(best_predictions.values, predictions.values, labels, n_tests=N_TESTS, sample_size=SAMPLE_SIZE, metric_fun=f1_score, metric_name=\"Macro F1\", metric_params={\"average\": \"macro\", \"zero_division\": 0})\n",
    "            print(\"----------------------------------------------------------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HLT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
