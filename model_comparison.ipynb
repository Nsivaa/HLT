{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.dataset_utils import *\n",
    "import pickle\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from lib.scores import *\n",
    "from lib.plot_utils import *\n",
    "from lib.models import bootstrap_test\n",
    "from sklearn.metrics import f1_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS = [\"Bayes\", \"DecisionTree\", \"RandomForest\", \"Bert\", \"Roberta\", \"Llama3\"]\n",
    "DATASETS = [\"GoEmotions\", \"TwitterData\", \"GoEmotionsGrouped\"]\n",
    "BASE_MODELS_DIR = \"./checkpoints/\"\n",
    "\n",
    "GOEMOTIONS_MODELS_PATH = {\n",
    "    \"Bayes\": f\"{BASE_MODELS_DIR}GoEmotions/nb_classifier.pkl\",\n",
    "    \"DecisionTree\": f\"{BASE_MODELS_DIR}GoEmotions/dt_classifier.pkl\",\n",
    "    \"RandomForest\": f\"{BASE_MODELS_DIR}GoEmotions/rf_classifier.pkl\",\n",
    "    \"Bert\": f\"{BASE_MODELS_DIR}GoEmotions/bert_model.pth\",\n",
    "    \"Roberta\": f\"{BASE_MODELS_DIR}GoEmotions/GoEmotions_Roberta.pth\"\n",
    "}\n",
    "\n",
    "GOEMOTIONS_GROUPED_MODELS_PATH = {\n",
    "    \"Bayes\": f\"{BASE_MODELS_DIR}GoEmotions/GoEmotions_goruped_bayes.pkl\",\n",
    "    \"DecisionTree\": f\"{BASE_MODELS_DIR}GoEmotions/GoEmotions_goruped_dt.pkl\",\n",
    "    \"RandomForest\": f\"{BASE_MODELS_DIR}GoEmotions/GoEmotions_goruped_rf.pkl\",\n",
    "    \"Bert\": f\"{BASE_MODELS_DIR}GoEmotions/grouped_bert_model.pth\",\n",
    "    \"Roberta\": f\"{BASE_MODELS_DIR}GoEmotions/GoEmotions_grouped_Roberta.pth\"\n",
    "}\n",
    "\n",
    "TWITTER_MODELS_PATH = {\n",
    "    \"Bayes\": f\"{BASE_MODELS_DIR}TwitterData/nb_classifier.pkl\",\n",
    "    \"DecisionTree\": f\"{BASE_MODELS_DIR}TwitterData/dt_classifier.pkl\",\n",
    "    \"RandomForest\": f\"{BASE_MODELS_DIR}TwitterData/rf_classifier.pkl\",\n",
    "    \"Bert\": f\"{BASE_MODELS_DIR}TwitterData/bert_model.pth\",\n",
    "    \"Roberta\": f\"{BASE_MODELS_DIR}TwitterData/Twitter_Roberta.pth\"\n",
    "}\n",
    "\n",
    "DATASET_TO_PATH_DICT = {\n",
    "    \"GoEmotions\": GOEMOTIONS_MODELS_PATH,\n",
    "    \"TwitterData\": TWITTER_MODELS_PATH,\n",
    "    \"GoEmotionsGrouped\": GOEMOTIONS_GROUPED_MODELS_PATH\n",
    "}\n",
    "\n",
    "DATASET_TO_ENUM_DICT = {\n",
    "    \"GoEmotions\": DatasetEnum.GoEmotionsCleaned,\n",
    "    \"TwitterData\": DatasetEnum.TwitterDataCleaned,\n",
    "    \"GoEmotionsGrouped\": DatasetEnum.GoEmotionsCleaned\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bayes_predict(dataset):\n",
    "    # file path\n",
    "    model_path = DATASET_TO_PATH_DICT[dataset][\"Bayes\"]\n",
    "    # load model\n",
    "    with open(model_path, 'rb') as f:\n",
    "        model = pickle.load(f)\n",
    "    # load test data\n",
    "    _, _, test_df = load_dataset(DATASET_TO_ENUM_DICT[dataset])\n",
    "    # predict\n",
    "    predictions = model.predict(test_df['text'])\n",
    "    # transform to dataframe\n",
    "    predictions_df = pd.DataFrame(predictions, columns=test_df.columns[1:])\n",
    "    return predictions_df\n",
    "\n",
    "def clean_content(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    # tokenize\n",
    "    clean_text = nltk.word_tokenize(text)\n",
    "    # pos tag\n",
    "    clean_text = nltk.pos_tag(clean_text)\n",
    "    TAG_MAP = {'N': 'n', 'V': 'v', 'R': 'r', 'J': 'a'}\n",
    "    clean_text = [(word, TAG_MAP.get(tag[0], 'n')) for word, tag in clean_text]\n",
    "    # lemmatize\n",
    "    clean_text = [lemmatizer.lemmatize(word, tag) for word, tag in clean_text]\n",
    "    # remove punctuation marks\n",
    "    clean_text = [w for w in clean_text if w not in string.punctuation]\n",
    "    return ' '.join(clean_text)\n",
    "\n",
    "def clean_df(df, text_col, out_col):\n",
    "    df[out_col] = df[text_col].apply(clean_content)\n",
    "    return df\n",
    "\n",
    "def decision_tree_predict(dataset):\n",
    "    # file path\n",
    "    model_path = DATASET_TO_PATH_DICT[dataset][\"DecisionTree\"]\n",
    "    # load model\n",
    "    with open(model_path, 'rb') as f:\n",
    "        model = pickle.load(f)\n",
    "    # load test data\n",
    "    _, _, test_df = load_dataset(DATASET_TO_ENUM_DICT[dataset])\n",
    "    # apply cleaning\n",
    "    predict_df = clean_df(test_df, 'text', 'text')\n",
    "    # predict\n",
    "    predictions = model.predict(predict_df['text'])\n",
    "    # transform to dataframe\n",
    "    predictions_df = pd.DataFrame(predictions, columns=predict_df.columns[1:])\n",
    "    return predictions_df\n",
    "\n",
    "def random_forest_predict(dataset):\n",
    "    # file path\n",
    "    model_path = DATASET_TO_PATH_DICT[dataset][\"RandomForest\"]\n",
    "    # load model\n",
    "    with open(model_path, 'rb') as f:\n",
    "        model = pickle.load(f)\n",
    "    # load test data\n",
    "    _, _, test_df = load_dataset(DATASET_TO_ENUM_DICT[dataset])\n",
    "    # apply cleaning\n",
    "    predict_df = clean_df(test_df, 'text', 'text')\n",
    "    # predict\n",
    "    predictions = model.predict(predict_df['text'])\n",
    "    # transform to dataframe\n",
    "    predictions_df = pd.DataFrame(predictions, columns=predict_df.columns[1:])\n",
    "    return predictions_df\n",
    "\n",
    "def bert_predict(dataset):\n",
    "    # file path\n",
    "    model_path = DATASET_TO_PATH_DICT[dataset][\"Bert\"]\n",
    "    # load model\n",
    "    model = torch.load(model_path)\n",
    "    # load test data\n",
    "    _, val_df, test_df = load_dataset(DATASET_TO_ENUM_DICT[dataset])\n",
    "    # predict on validation data\n",
    "    predictions = model.predict(val_df)\n",
    "    thresh, _ = tune_sigmoid_threshold(val_df[1:], predictions, f1_score, metric_params={\"average\": \"macro\", \"zero_division\": 0})\n",
    "    # predict on test data\n",
    "    predictions = model.predict(test_df)\n",
    "    # apply threshold\n",
    "    predictions = (predictions > thresh).astype(int)\n",
    "    # transform to dataframe\n",
    "    predictions_df = pd.DataFrame(predictions, columns=test_df.columns[1:])\n",
    "    return predictions_df\n",
    "\n",
    "def roberta_predict(dataset):\n",
    "    # file path\n",
    "    model_path = DATASET_TO_PATH_DICT[dataset][\"Roberta\"]\n",
    "    # load model\n",
    "    model = torch.load(model_path)\n",
    "    # load test data\n",
    "    _, val_df, test_df = load_dataset(DATASET_TO_ENUM_DICT[dataset])\n",
    "    # predict on validation data\n",
    "    predictions = model.predict(val_df)\n",
    "    thresh, _ = tune_sigmoid_threshold(val_df[1:], predictions, f1_score, metric_params={\"average\": \"macro\", \"zero_division\": 0})\n",
    "    # predict on test data\n",
    "    predictions = model.predict(test_df)\n",
    "    # apply threshold\n",
    "    predictions = (predictions > thresh).astype(int)\n",
    "    # transform to dataframe\n",
    "    predictions_df = pd.DataFrame(predictions, columns=test_df.columns[1:])\n",
    "    return predictions_df\n",
    "\n",
    "def llama3_predict(dataset):\n",
    "    # load predictions csv\n",
    "    predictions_df = pd.read_csv(f\"./results/llama3_{dataset}_predictions.csv\")\n",
    "    return predictions_df\n",
    "\n",
    "PREDICTOR_DICT = {\n",
    "    \"Bayes\": bayes_predict,\n",
    "    \"DecisionTree\": decision_tree_predict,\n",
    "    \"RandomForest\": random_forest_predict,\n",
    "    \"Bert\": bert_predict,\n",
    "    \"Roberta\": roberta_predict,\n",
    "    \"Llama3\": llama3_predict\n",
    "}\n",
    "\n",
    "def predict(model, dataset):\n",
    "    return PREDICTOR_DICT[model](dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_classification_report(scores_dict, labels_list):\n",
    "    # print classification report\n",
    "    print(\"Classification Report:\")\n",
    "    # print header\n",
    "    print(f\"{'Label':<20}{'Precision':<20}{'Recall':<20}{'F1-Score':<20}{'Jaccard':<20}{'Support':<20}\")\n",
    "    # print scores for each label\n",
    "    for label in labels_list:\n",
    "        print(f\"{label:<20}{scores_dict[label]['precision']:<20}{scores_dict[label]['recall']:<20}{scores_dict[label]['f1-score']:<20}{scores_dict[label]['jaccard']:<20}{scores_dict[label]['support']:<20}\")\n",
    "    # print aggregated scores\n",
    "    print(f\"{'Macro avg':<20}{scores_dict['macro avg']['precision']:<20}{scores_dict['macro avg']['recall']:<20}{scores_dict['macro avg']['f1-score']:<20}{scores_dict['jaccard']['macro']:<20}{scores_dict['macro avg']['support']:<20}\")\n",
    "    print(f\"{'Micro avg':<20}{scores_dict['micro avg']['precision']:<20}{scores_dict['micro avg']['recall']:<20}{scores_dict['micro avg']['f1-score']:<20}{scores_dict['jaccard']['micro']:<20}{scores_dict['micro avg']['support']:<20}\")\n",
    "    print(f\"{'Weighted avg':<20}{scores_dict['weighted avg']['precision']:<20}{scores_dict['weighted avg']['recall']:<20}{scores_dict['weighted avg']['f1-score']:<20}{scores_dict['jaccard']['weighted']:<20}{scores_dict['weighted avg']['support']:<20}\")\n",
    "    # print membership score and jaccard samples\n",
    "    print(f\"{'Membership Score':<20}{scores_dict['membership']:<20}\")\n",
    "    print(f\"{'Jaccard Samples':<20}{scores_dict['jaccard']['samples']:<20}\")\n",
    "\n",
    "def get_model_scores(model, dataset):\n",
    "    predictions = predict(model, dataset)\n",
    "    _, _, test_df = load_dataset(DATASET_TO_ENUM_DICT[dataset])\n",
    "    labels_list = test_df.columns[1:]\n",
    "    # collect scores in a dictionary\n",
    "    scores = classification_report(test_df[labels_list].values, predictions.values, target_names=labels_list, output_dict=True)\n",
    "    # add additional metrics\n",
    "    # compute jaccard scores\n",
    "    to_add = jaccard_score(test_df[labels_list].values, predictions.values, zero_division=0, average=None)\n",
    "    for i, label in enumerate(labels_list):\n",
    "        scores[label]['jaccard'] = to_add[i]\n",
    "    # add aggregated jaccard scores\n",
    "    scores['jaccard']['samples'] = jaccard_score(test_df[labels_list].values, predictions.values, zero_division=0, average='samples')\n",
    "    scores['jaccard']['macro'] = jaccard_score(test_df[labels_list].values, predictions.values, zero_division=0, average='macro')\n",
    "    scores['jaccard']['micro'] = jaccard_score(test_df[labels_list].values, predictions.values, zero_division=0, average='micro')\n",
    "    scores['jaccard']['weighted'] = jaccard_score(test_df[labels_list].values, predictions.values, zero_division=0, average='weighted')\n",
    "    # add membership score\n",
    "    scores['membership'] = membership_score(test_df[labels_list].values, predictions.values)\n",
    "    return scores\n",
    "\n",
    "def print_results(model_name, dataset_name, predictions_df, targets_df):\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"Dataset: {dataset_name}\")\n",
    "    plot_multilabel_confusion_heatmap(targets_df.values, predictions_df.values, label_true=targets_df.columns, label_pred=targets_df.columns, normalize=True)\n",
    "    # bar plot over classes\n",
    "    plot_score_barplot(targets_df.values, predictions_df.values, targets_df.columns)\n",
    "    # compute scores\n",
    "    scores = get_model_scores(model_name, dataset_name)\n",
    "    # print scores\n",
    "    custom_classification_report(scores, targets_df.columns)\n",
    "    return scores\n",
    "\n",
    "def comparison_bar_plot(dataset):\n",
    "    scores_dict = {}\n",
    "    for model in MODELS:\n",
    "        scores = get_model_scores(model, dataset)\n",
    "        scores_dict[model] = scores['macro avg']['f1-score']\n",
    "    # create bar plot\n",
    "    plt.bar(scores_dict.keys(), scores_dict.values())\n",
    "    plt.xlabel(\"Model\")\n",
    "    plt.ylabel(\"F1-Score\")\n",
    "    plt.title(f\"Comparison of models on {dataset}\")\n",
    "    plt.show()\n",
    "\n",
    "def print_dataset_results(dataset):\n",
    "    scores = {}\n",
    "    for model in MODELS:\n",
    "        predictions_df = predict(model, dataset)\n",
    "        _, _, targets_df = load_dataset(DATASET_TO_ENUM_DICT[dataset])\n",
    "        cur_scores = print_results(model, dataset, predictions_df, targets_df[1:])\n",
    "        scores[model] = cur_scores\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance comparison on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print results for all models on all datasets\n",
    "all_scores = {}\n",
    "for dataset in DATASETS:\n",
    "    all_scores[dataset] = print_dataset_results(dataset)\n",
    "    comparison_bar_plot(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get best model for each dataset\n",
    "best_models = {}\n",
    "for dataset in DATASETS:\n",
    "    best_model = max(all_scores[dataset].items(), key=lambda x: x[1]['macro avg']['f1-score'])\n",
    "    best_models[dataset] = best_model\n",
    "    # compare best model with all other models according to bootstrap test\n",
    "    for model in MODELS:\n",
    "        if model != best_model[0]:\n",
    "            print(f\"Bootstrap test between {best_model[0]} and {model} on {dataset}\")\n",
    "            bootstrap_test(all_scores[dataset][best_model[0]], all_scores[dataset][model], f1_score, metric_params={\"average\": \"macro\", \"zero_division\": 0})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HLT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
